{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4e88026ce4ab7025bfceb49954582dcd",
     "grade": false,
     "grade_id": "cell-cd00c4135f26c3e7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Logistic Regression - Predicting shipwreck survival\n",
    "\n",
    "In this week's assignment you will implement a classification algorithm named Logistic Regression. This sounds very counter-intuitive, as regression is something entirely different from classification, but the Logistic Regression model is actually estimating the probability (which is a continuous value) that data can be assigned to a specific class out of a set of classes. We will focus on so-called binary classification problems, wherein there are two possible classes. Some of the examples of binary classification problems that Logistic Regression is able to solve are: \n",
    "\n",
    "- Email; spam or not spam \n",
    "- Online transactions; fraud or not fraud\n",
    "- Tumor; malignant or benign \n",
    "\n",
    "In this notebook, we will be using the classic Titanic dataset. This data consists of demographic and traveling information for 891 of the Titanic's passengers, and the goal is to predict which of these passengers survived. \n",
    "\n",
    "Here is a summary of the data set's attributes:\n",
    "\n",
    "- *PassengerId*: passenger ID assigned in this dataset\n",
    "- *Survived*: a Boolean indicating whether the passenger survived or not (0 = No; 1 = Yes); this is our target\n",
    "- *Pclass*: passenger class (1 = 1st; 2 = 2nd; 3 = 3rd)\n",
    "- *Name*: field containing the name and title of the passenger\n",
    "- *Sex*: male/female\n",
    "- *Age*: age of the passenger in years\n",
    "- *SibSp*: number of siblings/spouses aboard\n",
    "- *Parch*: number of parents/children aboard\n",
    "- *Ticket*: ticket number\n",
    "- *Fare*: passenger fare in British Pounds\n",
    "- *Cabin*: location of the cabin, consisting of a letter indicating the deck, and a cabin number.\n",
    "- *Embarked*: port of embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)\n",
    "\n",
    "As you can probably see, the dataset contains a mix of textual, continuous, categorical, and boolean variables. Before we can get into implementing and applying Logistic Regression, we will have to clean up the data. We will lead you through this process using functions a data-scientist might use, providing you with the tools that will help you prepare your own data in the future.\n",
    "\n",
    "Run the code below to load the data and display the first few rows of that data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ba03e848bbe3b39e4cc7380a398eae9c",
     "grade": false,
     "grade_id": "cell-4791260d8b155eb1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from notebook_checker import start_checks\n",
    "# Start automatic globals checks\n",
    "%start_checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6be428a6ec9f93910bdc88116d1b7212",
     "grade": false,
     "grade_id": "cell-d405d8cc5f521e9e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "data = pd.read_csv('titanic.csv')\n",
    "\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a9021f30928c6932c0dbad9cbbbe227f",
     "grade": false,
     "grade_id": "cell-4e8b6367bb5bc972",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### A quick view\n",
    "\n",
    "From the first five rows displayed in the table above, you might be able to see that there are multiple types of variables: we have integers, strings, floats, and a few *NaN*'s, which are missing values. Let's take a quick look to see what the types of data that we are dealing with actually are. A tool a data-scientist might use here is Panda's [`df.info()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.info.html), which is a method that can be applied to any `DataFrame` that shows information about that frame, including the index, datatype for each column, non-null values, and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9daa0f48c5a8177a498759c8755afaf9",
     "grade": false,
     "grade_id": "cell-44dfe9df823c274e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2e6ad4ce934ab2bcbb6a90fc48b1e825",
     "grade": false,
     "grade_id": "cell-5713405aa38f1328",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As you can see we have 891 entries, numbered 0 to 890, with 12 different features. Of these 12 features, *Survived* will be our target features. Out of 12 columns, 2 are floats, 5 are integers, and 5 are \"objects\" which is the generic type, but in this case these are strings. Out of all of the columns, there are 3 that have some missing values. We will need to fix these missing values later.\n",
    "\n",
    "Let's take a better look at what the numeric columns, i.e. those that contain integers and floats, look like. For this, we will use Panda's [`df.describe`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html?highlight=describe#pandas.DataFrame.describe), which is a method that can be applied to any `DataFrame` to show its descriptive statistics, which summarize the central tendency, dispersion and shape of a datasetâ€™s distribution, excluding NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "216442f85da9930c1c62f71435c41162",
     "grade": false,
     "grade_id": "cell-f650408bba9c6c6c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "display(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c9a2a70cdace502f3b8733da2e9d6bd9",
     "grade": false,
     "grade_id": "cell-2a950d3aba5d0324",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The result is a table showing some statistics for each of the columns. The `count` is the number of values that are not NaN in that specific column. We can also see the mean, standard deviation, minimum, maximum, and percentiles for each of the columns. Percentiles are covered in *SOWISO* chapter 2 (Describing Data), the $50^{th}$ percentile corresponds to the median value. Let's see what we can conclude from this table and the previous one:\n",
    "\n",
    "- *PassengerId* is a column that runs from 1 to 891, indicating the index of a passenger in this dataset. This variable has no descriptive value for the passenger, but it could still cause our machine learning model to find some correlations that are really just noise, so we should delete this whole column before using the data for training.\n",
    "- The mean of *Survived* indicates that out of the 891 people in our dataset, only ~38% of people survived. This can be deduced from the mean of the column, because the value 1 corresponds to survived and 0 corresponds to did not survive. As an example, when 90% of these values would be 1, we would end up with a mean of 0.9, as the other values are all zeros.\n",
    "- There are more people in *Passenger Class* 3 than in class 1 and 2 combined. This can be concluded from the fact that from the 50th percentile to the maximum value, every passenger is in third class.\n",
    "- There is at least one baby in our dataset, with an *Age* of 0.42, or 5 months. The oldest person is 80 years old. We can see this from the min and max values of the *Age* column.\n",
    "- There are 277 people of which we do not know the *Age*. We have 891 data entries, but the `count` for age is 714, so 891 - 714 = 277.\n",
    "- The majority of people has no *Siblings/Spouses* aboard. This can be concluded from the percentiles.\n",
    "- The majority of people has no *Parents/Children* aboard. This can be concluded from the percentiles.\n",
    "- There is a big difference in scale in data. Fare can be up to 512 pounds while the number of *Parents/Children* on board is only up to 6.  We can also see this in the standard deviations, where the difference is also very large. In his videos, Andrew Ng talks about feature scaling, which is something we should apply here. We will discuss this later.\n",
    "\n",
    "Often, these tables can tell you a lot about the data you are working with, and might provide you with intuitions on what would work best for this dataset. We will spend some more time on analysing the data at the end of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "19eea44a7556a302ac6cf8d1b6640be9",
     "grade": false,
     "grade_id": "cell-51f4e0cf439102b9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 1: Cleaning the data\n",
    "\n",
    "First let's remove feature from the data that we won't use in the Logistic Regression model. Uniquely identifying features are generally a bad idea to include in training data, as they will never generalize well to new samples. In theory, the model might learn that a passenger with a specific ID is less likely to survive, but that is only relevant to the specific training data used and unlikely to improve predictions for other samples. In order to create a model that will generalize better, we'll completely remove *PassengerId*, *Name* and *Ticket* from this data set.\n",
    "\n",
    "Create a new DataFrame called `clean_data` that is a direct copy of our original `data`, but does not include the columns *PassengerId*, *Name* and *Ticket*. Take a look at Panda's [`df.drop`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html) function, which can be used to delete columns from a given `DataFrame`, when setting the `axis` parameter correctly. The method returns a new `Dataframe` without the rows/columns that were deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "75b982a13cd07257fd4a07f6e2e950cb",
     "grade": true,
     "grade_id": "cell-af09d9f39951109f",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "clean_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b87cb51a83953002b9d5c03dffe9467f",
     "grade": false,
     "grade_id": "cell-a16875bb9dc0a139",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "If you did the step above correctly, the `clean_data` DataFrame should now only have 9 columns, as 3 were removed. Looking at this description, we can see there are quite a few missing values in the data set, which are indicated as *null* values. For example, the *Cabin* feature has $204$ non-null values, while the data set consists of $891$ entries, meaning there are $687$ values missing here.\n",
    "\n",
    "One could say that the absence of information is also information, but when training a machine learning model, most models cannot make any predictions when one of the input variables is missing, making these samples unusable. When you encounter missing values, you have multiple options for dealing with them:\n",
    "\n",
    "* You can delete each row (sample) that contains a missing value\n",
    "* You can delete the whole column (feature) containing the missing values\n",
    "* You can replace the missing values with some other value\n",
    "\n",
    "Which of these you choose depends on the specific feature, the size of your data set and number of values missing. For the *Cabin* feature, the majority of the values are actually missing, which makes dropping the feature entirely a good solution. Note that we are still throwing away some potentially useful information here, as a cabin number might converted to information on where the passenger was on the ship, which in turn might give information on if a passenger would have been able to reach a lifeboat on time. In a more advanced data processing pipeline we might still include some of this data, but that would include data mining techniques that are beyond the scope of this notebook.\n",
    "\n",
    "So, for now, we'll just remove the *Cabin* column completely, as for most of the samples it does not contain any information anyway. Again, use [`df.drop`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html), this time to remove the *Cabin* column from the `clean_data`. Remember to store the result into `clean_data` again, as the function returns the new DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "db08360fdd30cb096a21b2d337e4f82a",
     "grade": true,
     "grade_id": "cell-acca3db88c8a1a04",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "clean_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "68e393e398aaf8ccb2b296cd3b58d88c",
     "grade": false,
     "grade_id": "cell-2fcaf1fb4065e841",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Next, let's take a look the *Embarked* missing values. There appear to only be $2$ values missing here, as there are $889$ non-null values. The most common solution in such a case is to just remove those two samples from data set completely, as you'll only remove a few samples from your data, while retaining the *Embarked* information for most of them.\n",
    "\n",
    "For this you can use the function [`df.dropna`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html), which removes missing values from a DataFrame. As we only want to apply this solution to the *Embarked* column, make sure to use the optional argument `subset` here, which you can give a list of column names to include in the operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1b49bf6e68e7ab288abadc0e85e1c536",
     "grade": true,
     "grade_id": "cell-4e580a1ceeecbb00",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "clean_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "df016c33a539040edca291cb6e72edef",
     "grade": false,
     "grade_id": "cell-c1a3e214f3bbbf12",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The only remaining missing values should now be the $177$ passengers for which we do not know the age. Here, deleting the rows really isn't a great option, as we don't have a lot of data to start with, and we need as many training samples as possible to create an accurate model. Deleting the column also wouldn't be a good option, as there are many cases where we *do* have the age, and the age of a person will probably be a good indicator for whether a person survived or not. The third option, replacing the missing values, remains, but we have to find a method that will not skew our results. \n",
    "\n",
    "There are multiple ways of replacing missing values, but it is important to find a method that does not affect the outcome of our model too much. Replacing the values randomly will probably create a lot of noise and outliers, and therefore might prevent the model from learning any real correlations between age and survival of the passenger.  So, we will use a method that replaces missing values with values that are as generic as possible, as to not skew the data too much, by replacing all missing values with a value that is inferred from the data that is available. The easiest option for this is to just use the *mean* of all the non-missing values as the replacement value.\n",
    "\n",
    "In the cell below, use Panda's [`df.fillna`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html) function to replace every missing value in the *Age* column of `clean_data` with the average age based on all the non-missing values. You can use the [`mean()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mean.html) function on the *Age* column to compute the average age. Select the column *Age*, apply `fillna` to replace every missing value with the computed mean value and then store the result in the *Age* column to overwrite it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6d8676966b7f9dee1b741152f4530611",
     "grade": true,
     "grade_id": "cell-3d3e5281e0b1ebe8",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "clean_data.info()\n",
    "display(clean_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "18da98eca8c111e0aefb48c43224d7c0",
     "grade": true,
     "grade_id": "cell-b048564a1a879a56",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(clean_data) == 889, \"Something went wrong while cleaning the data. Did you remove the rows with no data for \\'embarked\\'?\" \n",
    "assert not clean_data.isnull().any().any(), \"Something went wrong while cleaning the data. There are still missing values in your dataframe.\"\n",
    "print(\"Solution seems correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8b6375d3a071e78c8e97ff83e61d9bde",
     "grade": false,
     "grade_id": "cell-7d3afa738d85518d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As you can see, all missing values in the column *Age* have now been replaced with the average. This means that the average of the column has not changed. However, this solution is still not ideal. We have changed the standard deviation and the percentiles, which indicates that we have changed the entire distribution of the data. As there is no way to recover the real data, for now we will settle for this solution.\n",
    "\n",
    "Finally, we will have to transform the columns *Sex* and *Embarked* from categorical data into a numerical type that the Logistic Regression can work with. The column *Sex* has two categories, male or female, while the port where passengers *Embarked* has three categories; C = Cherbourg, Q = Queenstown, or S = Southampton. In the previous module, you learned about one-hot encoding, which is exactly what we will use here too. Pandas has a method named [`get_dummies`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html) that transforms data from categorical to a one-hot encoded set of columns.\n",
    "\n",
    "Note that, for now, we will choose to interpret the *Passenger Class* as a numeric variable, as we won't one-hot encode it. However, you could make the argument that it is actually a categorical variable, as it only really has three values. In this case, both interpretations of the variable are valid, as it is a categorical variable with ordinal properties; second class is actually something \"between\" first and third class. Using the variable as if it is numerical, however, *does* imply that the difference between first and second class is the same as the difference between second and third class. Using the variable as if it is categorical implies that there is no order, and that the three different classes have no connection other than that you can only be part of one class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "00e6ff00795368b1e2286386e3d0aded",
     "grade": false,
     "grade_id": "cell-e5ef881b3f1ad65a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create two new columns for the male and female one-hot encoding\n",
    "sex = pd.get_dummies(clean_data['Sex'])\n",
    "\n",
    "# Create three new columns for port of embarkment one-hot encoding\n",
    "embark = pd.get_dummies(clean_data['Embarked'])\n",
    "\n",
    "# We no longer need these columns, as we will replace them soon\n",
    "clean_data = clean_data.drop(['Sex','Embarked'], axis=1)\n",
    "\n",
    "# Create the new dataframe with the one-hot columns we created from the categories in Sex and Embarked\n",
    "clean_data = pd.concat([clean_data, sex, embark], axis=1)\n",
    "\n",
    "clean_data.info()\n",
    "display(clean_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f5787cae6cc56d1f5955d7f31d6fff76",
     "grade": false,
     "grade_id": "cell-14d84ebcd9a34110",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Final preparations\n",
    "\n",
    "Now that all data is numerical, we can continue to the next step in processing the data. Earlier, we already observed that there is a big difference in the scale of each of the features in the data, which can impact the learning process for a model (see the theory video for more details). Feature scaling can be used to ensure that the different features in your dataset take on similar ranges of values.\n",
    "\n",
    "Scipy's `stat` module has some interesting methods to do this, including the `zscore` method which you might also recognise from the theory video:\n",
    "\n",
    "$$ \\tilde{\\mathbf{x}}_j = \\frac{\\mathbf{x}_j - \\mu_j}{\\sigma_j} $$\n",
    "\n",
    "Here $\\mathbf{x}_j$ is a vector of x-values for the $j^{th}$ column, $\\mu_j$ and $\\sigma_j$ are the (scalar) mean and standard deviation for that same column, and $\\tilde{\\mathbf{x}}_j$ is the scaled version of the feature vector. The basic principle here is that it transforms every value in the column $j$ to a value that indicates how many standard deviations the value originally was away from the mean of the data *in that column*. For example: a $Z$-score of $-2$ would indicate that the value was 2 standard deviations below the average, while a $Z$-score of $0$ would indicate that the value was actually equal to the mean of the column.\n",
    "\n",
    "In the code below, we transform each of the original numerical data columns using the `zscore` method, by applying it to each of the elements in those columns. The function `zscore` conveniently takes care of computing all means and standard deviations for each column separately, and returns the scaled data.\n",
    "\n",
    "Note that we do not want to apply this function to our newly one-hot encoded data, as those are already within a nice $[0,1]$ range and scaling these binary values by their mean and standard deviation wouldn't really make sense here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "62ec7bd8e6ea0072dd625e0eba6b6460",
     "grade": false,
     "grade_id": "cell-c9b01eace430b7f8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "numerical = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\n",
    "clean_data[numerical] = clean_data[numerical].apply(zscore)\n",
    "\n",
    "display(clean_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "97851a763fae1a6aa21f4e5d9ed2f888",
     "grade": false,
     "grade_id": "cell-8098df476f13eed4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The columns that we have adjusted with our `zscore` function now all have a mean that is very close to zero, and a standard deviation that is very close to one. So, the resulting data is now sufficiently of equal scale, and mean-centered.\n",
    "\n",
    "With the data completely cleaned up, the final step is to put the data into format useful for learning. This means splitting the data into the input features `X` and the target variable `y`. In addition, we'll convert the data to *Numpy* `ndarrays` here, so we can again easily use vectorization in our Logistic Regression implementation. We've used Pandas for reading the data and cleaning it up (which is where *Pandas* excels), and now we'll use *Numpy* for the actual learning / computation portion. Finally, we'll need to also split the data set into a training and test set for learning, for which we'll use the `sklearn` function `train_test_split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c8fa84876eb6e7b0db2915f195ed3f1d",
     "grade": false,
     "grade_id": "cell-85dcdf2741c7c655",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into target \"y\" and input \"X\"\n",
    "y_df = clean_data['Survived']\n",
    "X_df = clean_data.drop('Survived', axis=1)\n",
    "\n",
    "# Convert the Pandas DataFrames to Numpy ndarrays\n",
    "X_np = X_df.to_numpy()\n",
    "y_np = y_df.to_numpy()\n",
    "\n",
    "#Split the data into 70% training and 30% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_np, y_np, train_size=0.7, random_state=1265599650)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b028cda029856262992b76052b0b8868",
     "grade": false,
     "grade_id": "cell-73f167f99d2d8ec5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 2: The Logistic function\n",
    "\n",
    "For Logistic Regression, we require a function that generates probabilities; a function that gives outputs between 0 and 1 for **all** inputs. There are many functions that meet this description, but the one that is used in logistic regression is the logistic function:\n",
    "\n",
    "$$ g(z) = \\frac{1}{1+e^{-z}} $$\n",
    "\n",
    "Implement the function `logistic_func` that can either take a single value `z`, or an array of values `z`, and compute the *Logistic* function for each.\n",
    "\n",
    "*Hint:* Numpy built-in functions and basic arithmetic operations work on both Numpy arrays and single values. When the function detects that its input is a Numpy array, the operation it applies will be applied to each element in the array separately, the result being a Numpy array of the same shape as the input. This is called *element-wise application*, and many mathematical operations have Numpy versions that support element-wise application. Write your `logistic_func` in such a way that it can compute $g$ values for scalars or for entire vectors.\n",
    "\n",
    "Then, write some code that can plot the results for the logistic function between $-5$ and $5$. Use Numpy's [`np.linspace`](https://numpy.org/doc/stable/reference/generated/numpy.linspace.html?highlight=linspace#numpy.linspace) to generate a range of $z$ values and use your `logistic_func` to calculate the corresponding $g$ values. Make sure the resulting plot looks correct, before moving on to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8f8168997fe1cc0a95729a81a5f98f50",
     "grade": true,
     "grade_id": "cell-8c0b7f412d0e2f27",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def logistic_func(z):\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "15f020360168e73ff44cf1e26ff86d96",
     "grade": false,
     "grade_id": "cell-95912d5eda3d7f0f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 3: Building the Logistic model\n",
    "\n",
    "Next, we can start working on the hypothesis function, which will generate a vector of class probabilities, given some matrix of inputs $X$ and a vector of weights $\\theta$. For logistic regression, we'll use the logistic function $g$ to transform the hypothesis we used in multivariate linear regression into a hypothesis function that only generates probabilities (i.e. results between 0 and 1):\n",
    "\n",
    "$$ h_\\theta(x^1) = g(\\theta^Tx^1)$$\n",
    "$$ h_\\theta(x^2) = g(\\theta^Tx^2)$$\n",
    "$$ h_\\theta(x^3) = g(\\theta^Tx^3)$$\n",
    "$$\\dots$$\n",
    "$$ h_\\theta(x^n) = g(\\theta^Tx^n)$$\n",
    "\n",
    "This is exactly the same hypothesis equation as for multivariate linear regression, just with the logistic function $g$ applied as the final step. *Note:* The notation can be a little confusing here, as $x^i$ is a vector and the default shape for vectors is a *column* vector, even if $x^i$ in the complete data matrix $X$ is a *row* vector. This will actually get somewhat easier when we write out the full vectorized version using the matrix $X$, as the rows and columns get their \"expected\" shape. \n",
    "\n",
    "Here the function $g$ is the Logistic function defined above. As you've just written `logistic_func` specifically to also work on vectors, if we construct a vector of $z$ values for the whole model at once, then we could also write the entire hypothesis function in a vectorized form:\n",
    "\n",
    "$$ \\left[\\begin{array}{cccc}\n",
    "x_0^1 & x_1^1 & x_2^1 & \\cdots & x_n^1 \\\\ \n",
    "x_0^2 & x_1^2 & x_2^2 & \\cdots & x_n^2 \\\\ \n",
    "x_0^3 & x_1^3 & x_2^3 & \\cdots & x_n^3 \\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_0^m & x_1^m & x_2^m & \\cdots & x_n^m \\\\ \n",
    "\\end{array} \\right]\n",
    "\\left[\\begin{array}{c} \\theta_0 \\\\ \\theta_1 \\\\\n",
    "\\theta_2 \\\\ \\vdots \\\\ \\theta_n \\end{array} \\right]\n",
    "= \\left[\\begin{array}{cccc}\n",
    "z^1\\\\\n",
    "z^2\\\\\n",
    "z^3\\\\\n",
    "\\dots \\\\\n",
    "z^m \\\\\n",
    "\\end{array} \\right]\n",
    "$$\n",
    "\n",
    "Next, we can apply the Logistic function to each of the elements of this $\\mathbf{z}$ vector:\n",
    "\n",
    "$$\n",
    "\\left[\\begin{array}{cccc}\n",
    "g(z^1)\\\\\n",
    "g(z^2)\\\\\n",
    "g(z^3)\\\\\n",
    "\\dots \\\\\n",
    "g(z^m) \\\\\n",
    "\\end{array} \\right]\n",
    "=\n",
    "\\left[\\begin{array}{c} h_{\\theta}(x^1) \\\\ h_{\\theta}(x^2) \\\\\n",
    "h_{\\theta}(x^3) \\\\ \\vdots \\\\ h_{\\theta}(x^m) \\end{array} \\right]\n",
    "$$\n",
    "\n",
    "Large parts of these equations should look familiar, as they're really just *Multivariate Linear Regression* with an extra *Logistic function* applied. This means you can (and should) take inspiration from your multivariate linear solutions, although some of the details will be a little different here.\n",
    "\n",
    "First, write a function `add_x0`, which takes a matrix `X` and prepends a column vector of constant $1$ values to this matrix, returning this new matrix. Next, write the function `logistic_model` to compute the hypothesis vector of the model for some matrix `X` (which already has $x_0$ added, so no need to add it again) and parameter vector `theta`. Your solution for `logistic_model` should rely on `logistic_func` working for vectors as well as scalars, and so you shouldn't need any loops here at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b97654edcccf266607410813a794d6bc",
     "grade": true,
     "grade_id": "cell-f85aebe61e096d04",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def add_x0(X):\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "\n",
    "def logistic_model(X, theta):\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "\n",
    "X_train_x0 = add_x0(X_train)\n",
    "X_test_x0 = add_x0(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2f758a7c43daa24d0590dfd51193a81b",
     "grade": false,
     "grade_id": "cell-956c066c05eaf45c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 4: The Cost function\n",
    "\n",
    "Now that we can make predictions using an input matrix $X$, and our parameter weights $\\theta$, we can evaluate the cost of our model. To be able to apply gradient descent, we will need a cost function that is convex when we use our logistic model to make predictions. This cost function for logistic regression is explained in detail in the theory videos, and is defined as:\n",
    "\n",
    "$$J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^m y^i log(h_\\theta(X^i)) + (1 - y^i) log(1 - h_\\theta(X^i))$$\n",
    "\n",
    "Implement the function `logistic_cost`. Use your `logistic_model` function to calculate the hypothesis vector $h_\\theta(X)$. Just as with the cost function for *Multivariate Linear Regression*, it is actually possible to write this whole function using only linear algebra, and some element-wise function applications, although this one is a little harder.\n",
    "\n",
    "*Hints:* Most mathematical functions have a *Numpy* version that supports element-wise application, which will really help here. If you're stuck on how to write all of this as matrix multiplications, you can also use the element-wise version of multiplication [np.multiply](https://numpy.org/doc/stable/reference/generated/numpy.multiply.html). Addition and subtraction are already element-wise by default, so all that then remains is summing the resulting vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c87f2368123470f61655ad5d70a176f6",
     "grade": true,
     "grade_id": "cell-a7adcc17e69f48ac",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def logistic_cost(theta, X, y):\n",
    "    # YOUR CODE HERE\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5a1706533a9fd9fe791be2a645799c06",
     "grade": true,
     "grade_id": "cell-ba888670d9338422",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.allclose(logistic_cost(np.ones(X_train_x0.shape[1]), X_train_x0, y_train), 2.013825365789087), 'Cost for training data seems incorrect'\n",
    "print(\"Solution seems correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 5: Obtaining the Gradient terms for Logistic Regression\n",
    "\n",
    "Manually taking the whole partial derivative of this cost function can be a little time consuming, but in order to still practice this skill, we'll just do a part of this derivative instead. For this part of the notebook, you should consult the `logistic_derivative.pdf` supplement, included as a part of the assignment. Read sections 1, 2 and 3 in the supplement, and then complete the steps of the partial derivative below, labeling each of the steps with their respective rules (as with the linear regression assignment).\n",
    "\n",
    "$$\\frac{\\partial E_{\\mathbf{\\theta}}^i}{\\partial h_{\\mathbf{\\theta}}^i} =\n",
    "\\frac{\\partial}{\\partial h_{\\mathbf{\\theta}}^i} \\bigl(-y^i log(h_{\\mathbf{\\theta}}^i) -\n",
    "(1 - y^i) log(1 - h_{\\mathbf{\\theta}}^i) \\bigr)$$\n",
    "\n",
    "**TODO:** *Your steps, each labeled with what rules you've applied, should go here*.\n",
    "\n",
    "$$\\frac{\\partial E_{\\mathbf{\\theta}}^i}{\\partial h_{\\mathbf{\\theta}}^i} =\n",
    "\\frac{-y^i}{h_{\\mathbf{\\theta}}^i} +\n",
    "\\frac{1 - y^i}{1 - h_{\\mathbf{\\theta}}^i}$$\n",
    "\n",
    "Section 4 of the supplement is optional, but definitely read sections 5 and 6 before moving on to the next step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b8452a67231adc11985f367617ccead6",
     "grade": false,
     "grade_id": "cell-5840f59b60fd1789",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 6: Gradient Vector and Gradient Descent\n",
    "\n",
    "In the end, the partial derivatives for each of the $\\theta$ parameters look exactly the same as for polynomial regression. While the applied hypothesis function $h$ changes, the rest of the derivative does not, even though the cost function is completely different. As a result, the equations for the $n+1$ partial derivatives for each of the $n+1$ parameters, in the notation of the theory videos, then just become:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\theta_0} = \\frac{1}{m} \\sum_{i=1}^m (h_{\\theta}(x^i) - y^i)x^i_0$$\n",
    "$$\\frac{\\partial J}{\\partial \\theta_1} = \\frac{1}{m} \\sum_{i=1}^m (h_{\\theta}(x^i) - y^i)x^i_1$$\n",
    "$$\\frac{\\partial J}{\\partial \\theta_2} = \\frac{1}{m} \\sum_{i=1}^m (h_{\\theta}(x^i) - y^i)x^i_2$$\n",
    "$$\\dots$$\n",
    "$$\\frac{\\partial J}{\\partial \\theta_n} = \\frac{1}{m} \\sum_{i=1}^m (h_{\\theta}(x^i) - y^i)x^i_n$$\n",
    "\n",
    "Write the functions `gradient_vector` and `gradient_descent` for this *Logistic Regression* model. You should be able to reuse a lot of your code from *Multivariate Linear Regression* here, as a large part of this code actually hasn't changed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6a51a4166a358e54bc042056ed762887",
     "grade": true,
     "grade_id": "cell-5ed097690674d897",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def gradient_vector(theta, X, y):\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "def gradient_descent(X, y, theta, alpha, thres=10**-6):\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "\n",
    "# Initialize theta to a zero vector of the correct shape\n",
    "theta = np.zeros(X_train_x0.shape[1])\n",
    "\n",
    "# Find the theta vector that minimizes the cost function\n",
    "theta_hat = gradient_descent(X_train_x0, y_train, theta, 10**-4)\n",
    "\n",
    "print(theta_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "77c55b990b557fbf7f3d6b8e4d14deb8",
     "grade": true,
     "grade_id": "cell-3a14f7f3a8ee655f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.allclose(theta_hat, np.array([-0.17289611, -0.42245195, -0.16952217, -0.11465412, 0.08995297, 0.24693307, 0.47087209, -0.64376821, 0.05535226, 0.00347353, -0.2317219])), 'Optimal theta vector for training data seems incorrect'\n",
    "print(\"Solution seems correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a35cd70d5259d519b59c922b8c6e0420",
     "grade": false,
     "grade_id": "cell-216a350a1996b86b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 7: Inspecting the learning curves\n",
    "\n",
    "While this model probably converges with the provided parameters, it is hard to tell if gradient descent actually converged to a *good* solution. In order to assess this, we'll make a modified version of gradient descent that plots the learning curves. Learning curves show how the cost changes with each step of gradient descent, and plotting them can tell you a lot about how well an algorithm converged.\n",
    "\n",
    "You can plot the learning curve just for the training cost, where ideally you'd see the cost go down for each step (otherwise the algorithm is diverging) and eventually the cost would level off to almost flat, indicating that gradient descent cannot improve the parameters much further. You can also measure the testing or the validation cost at each step and see if this follows the same trend down as the training cost. If the training cost is going down, but your validation cost is going up, then your model must be overfitting.\n",
    "\n",
    "Complete the function `gradient_descent_learning_curves`, which should implement exactly the same gradient descent algorithm as in the previous assignment. In addition, it should store the training and testing cost at every step of the algorithm, and store the number of iterations gradient descent took to converge. Once the algorithm is converged, the function should plot the training and the testing cost as a function of the number of iterations, clearly labeling which line in the figure belongs to which cost. Finally, the function should return the converged result of the `theta` parameter vector, as before.\n",
    "\n",
    "The function will need a few additional arguments to determine the testing cost, namely `X_test` and `y_test`, so you should modify the function call accordingly. For this function call, you should also tweak the learning rate `alpha` and the convergence threshold `thres` and inspect the learning curves each time. Repeat this process and make sure you find a configuration you think has actually converged to a good solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6f7b87e65b4b052c208e651374bbe06e",
     "grade": true,
     "grade_id": "cell-e3ce49d28003f5b2",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def gradient_descent_learning_curves(X_train, y_train, X_test, y_test, theta, alpha, thres):\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "\n",
    "theta = np.zeros(X_train_x0.shape[1])\n",
    "    \n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3b5f34afc5f9ef20f3b2983f3fcd938f",
     "grade": false,
     "grade_id": "cell-82b0a52cad611042",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Q1. Explain which values you chose for the learning rate and convergence threshold of gradient descent. What happened to the learning curve when you changed these values?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "77d2d8833e006060273fbf88abfad63a",
     "grade": true,
     "grade_id": "cell-2d0ead0fb0a838ee",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "210106068aa712961f97661a4c74b1cb",
     "grade": false,
     "grade_id": "cell-ef7f78b9e0c6eeb5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Q2. For this model, it is actually not possible to overfit, no matter how much you tweak the learning rate and convergence threshold. Explain why a Logistic Regression model with these inputs could never overfit the data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d6138a0e22817bc9623922d1b95d51d6",
     "grade": true,
     "grade_id": "cell-3780352b323c225e",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a6788f887cf1dbf08cae31150e30bb8f",
     "grade": false,
     "grade_id": "cell-01e6efb34180b1f6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 8: Predictions\n",
    "\n",
    "Now that we have a learned vector of weights $\\theta$, we can use these weights to make predictions for our test samples `X_test`. The learned hypothesis function actually estimates the probability of the passengers in `X_test` to have survived the shipwreck or not.\n",
    "\n",
    "$$P(y^i=1|x^i,\\theta) = h_{\\theta}(x^i)$$\n",
    "\n",
    "Finally, these estimates of the probability that $y=1$ still need to be transformed into boolean predictions, which will be the actual classifications. In the end, this regression model should still predict that either the person has survived $y=1$, or that the person has not survived $y=0$.\n",
    "\n",
    "Write the function `predict`, that takes a matrix of input values `X` and a vector of weights `theta`, and transforms them into a vector of boolean predictions. Use `logistic_model` to generate predictions, and then use a decision boundary of $h_\\theta(x) \\geq 0.5$ to determine when you should classify each sample as $y=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cc01c53678a903414d9b716a7c3b1b43",
     "grade": true,
     "grade_id": "cell-18d952a8d830176f",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def predict(X, theta):\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "\n",
    "predictions_train = predict(X_train_x0, theta_hat)\n",
    "predictions_test = predict(X_test_x0, theta_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9d06816eba3e4a0165eaade592ad79e6",
     "grade": false,
     "grade_id": "cell-bda3d9fa55e80d8c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 9: Determining accuracy\n",
    "\n",
    "Next, we would like to see how *accurate* our model is, now that we have trained it and it can generate predictions. Implement the function `calc_accuracy` that accepts a vector of `predictions` and a vector of *ground-truth* values `y`. The function should return the ratio of correct predictions (predictions where the value in `predictions` is equal to the value in `y`). The ideal situation where every classification is correct, should result in an accuracy of $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d37f435efe09d376efbd8d6f69ae6a22",
     "grade": true,
     "grade_id": "cell-b57fd37e12d5bf09",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def calc_accuracy(predictions, y):\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "\n",
    "print(f\"Training set accuracy: {calc_accuracy(predictions_train, y_train)}\")\n",
    "print(f\"Testing set accuracy: {calc_accuracy(predictions_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e9635a0500956ca236453b9ef108132d",
     "grade": true,
     "grade_id": "cell-d7ef711737fbe0e5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.allclose(calc_accuracy(predict(X_test_x0, np.array([-0.17289611, -0.42245195, -0.16952217, -0.11465412, 0.08995297, 0.24693307, 0.47087209, -0.64376821, 0.05535226, 0.00347353, -0.2317219])), y_test), 0.7565543071161048), 'Accuracy for testing data seems incorrect'\n",
    "print(\"Solution seems correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b47d334dc0361eee9a81ecc99aa87fd0",
     "grade": false,
     "grade_id": "cell-475c820a28ac14e4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 10: Confusion matrix\n",
    "\n",
    "The accuracy we have just determined is, of course, one of the primary goals of building a model; we generally want a Machine Learning model to be as accurate as possible. However, it does not give much insight into in what way our model was accurate. As an example, let's say that we want to diagnose whether a patient has a virus. Of course we want our model to diagnose as accurately as possible, but there are two types of error the model could make:\n",
    "\n",
    "1. A patient *has* the virus, but was diagnosed as being negative\n",
    "2. A patient *does not have* the virus, but was diagnosed as being positive\n",
    "\n",
    "One of these types of mistakes is potentially *much* more costly than the other, as falsely diagnosing a patient as negative could become very harmful for the patient and their environment. Although an extreme (and currently very relevant) example, it is quite often the case in classification that not every error has the same associated *cost*.\n",
    "\n",
    "To give better insight into the performance of a classification algorithm, typically, a confusion matrix is made. The confusion matrix is a table layout that allows quick identification of the performance of a classification algorithm. Each *row* in the matrix represents the data that got predicted as a specific class, while each *column* represents the data that is actually in a class:\n",
    "\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr> <th colspan=2></th><th colspan=2 style=\"border: 1px solid black;\"> Actual class </th> </tr>\n",
    "        <tr>\n",
    "            <th colspan=2></th>\n",
    "            <th style=\"border: 1px solid black;\">P</th>\n",
    "            <th style=\"border: 1px solid black;\">N</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td rowspan=2 style=\"border: 1px solid black;\"><b> Predicted class </b></td>\n",
    "            <td style=\"border: 1px solid black;\"><b> P </b></td>\n",
    "            <td style=\"border: 1px solid black;\"><b> TP </b></td>\n",
    "            <td style=\"border: 1px solid black;\"> FP </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"border: 1px solid black;\"><b> N </b></td>\n",
    "            <td style=\"border: 1px solid black;\">FN</td>\n",
    "            <td style=\"border: 1px solid black;\"><b> TN </b></td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "Where:\n",
    "- P = Positive\n",
    "- N = Negative\n",
    "- TP = True Positive, denoting every value that was correctly predicted as positive\n",
    "- FP = False Positive, denoting every value that was predicted as positive but was actually negative\n",
    "- TN = True Negative, denoting every value that was correctly predicted as negative\n",
    "- FN = False Negative, denoting every value that was predicted as negative but was actually positive\n",
    "\n",
    "For example, using the example of diagnosis of patients, let's say that 100 people take a test, and of these people, 80 actually have the virus. We have two different testing kits that are used to diagnose the patients, and these are the resulting confusion matrices:\n",
    "\n",
    "\n",
    "##### Testing kit 1\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr> <th colspan=2></th><th colspan=2 style=\"border: 1px solid black;\"> Actual class </th> </tr>\n",
    "        <tr>\n",
    "            <th colspan=2></th>\n",
    "            <th style=\"border: 1px solid black;\">P</th>\n",
    "            <th style=\"border: 1px solid black;\">N</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td rowspan=2 style=\"border: 1px solid black;\"><b> Predicted class </b></td>\n",
    "            <td style=\"border: 1px solid black;\"><b> P </b></td>\n",
    "            <td style=\"border: 1px solid black;\"><b> 56 </b></td>\n",
    "            <td style=\"border: 1px solid black;\"> 1 </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"border: 1px solid black;\"><b> N </b></td>\n",
    "            <td style=\"border: 1px solid black;\"> 24 </td>\n",
    "            <td style=\"border: 1px solid black;\"><b> 19 </b></td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "##### Testing kit 2\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr> <th colspan=2></th><th colspan=2 style=\"border: 1px solid black;\"> Actual class </th> </tr>\n",
    "        <tr>\n",
    "            <th colspan=2></th>\n",
    "            <th style=\"border: 1px solid black;\">P</th>\n",
    "            <th style=\"border: 1px solid black;\">N</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td rowspan=2 style=\"border: 1px solid black;\"><b> Predicted class </b></td>\n",
    "            <td style=\"border: 1px solid black;\"><b> P </b></td>\n",
    "            <td style=\"border: 1px solid black;\"><b> 62 </b></td>\n",
    "            <td style=\"border: 1px solid black;\"> 10 </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"border: 1px solid black;\"><b> N </b></td>\n",
    "            <td style=\"border: 1px solid black;\"> 18 </td>\n",
    "            <td style=\"border: 1px solid black;\"><b> 10 </b></td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "Now, we can learn a lot from these confusion matrices. First, the calculated accuracy of our first virus diagnosis testing kit is 77%, as 56 people are correctly diagnosed as positive, and 19 people are correctly diagnosed as negative.  The calculated accuracy for the second testing kit is 72%. However, using the first testing kit 24 people have been incorrectly diagnosed as negative, while for the second this was 18. Now, arguably, we would prefer to use testing kit 2, as even though the total accuracy of this testing kit is lower, this kit produces fewer (harmful) false negatives. \n",
    "\n",
    "Implement the function `confusion_matrix` that, given `predictions` and truth values `y`, creates a $2 \\times 2$ Numpy matrix that contains the number of TP, FP, FN, and TN for these predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "23d16c74b75832877cc4117d17d85642",
     "grade": true,
     "grade_id": "cell-fc8db32602107484",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(predictions, y):\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "\n",
    "print(confusion_matrix(predictions_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "770df0733aa559d5c13b10d65b4aacff",
     "grade": false,
     "grade_id": "cell-bb4341a873dcc08f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Q3. Which type of error is larger for the Logistic Regression model on this Titanic data set; false positives or false negatives? Explain your answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "09be6c7c6965b3369e0bc13cc859a0c0",
     "grade": true,
     "grade_id": "cell-e9072e8694deab97",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "813df7c4d516aaca9724777dd866798b",
     "grade": false,
     "grade_id": "cell-74efcae9b4a18288",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Correlations and understanding weights\n",
    "\n",
    "To better understand what relations our model has found in the data we will have to take a look at the weights that our model has learned. The relative size of each weight is a direct indication of how important the corresponding feature is in defining the relationship between the input features and the target output. We can even argue why this is the case, as the hypothesis of our model is entirely dependant on the linear combination of the input values and our weights:\n",
    "\n",
    "$$ h_{\\theta}(x^i) = g(x^i_0 \\theta_0 + x^i_1 \\theta_1 + \\dots + x^i_n \\theta_n)$$\n",
    "\n",
    "The logistic function simply \"squashes\" the combined positive values to a maximum of 1, and the combined negative values to a minimum of 0, i.e. a negative weight means that a large value for this feature will actually decrease the chances of survival for a passenger. \n",
    "\n",
    "The input for our function $g$ is composed out of the sums of each input multiplied with its corresponding weight. Since we have made sure that our data all approximately has equal scale by using the `zscore` method, bigger values for a specific weight result in the input corresponding to that weight having more influence on the hypothesis. Similarly, negative values indicate that there is a negative correlation between the input variable and the hypothesis, while positive values indicate a positive correlation.\n",
    "\n",
    "Below, we have used Seaborn to create a barplot that shows each of the weights and their corresponding data column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e5d3193cf9c028f97a7e134493323252",
     "grade": false,
     "grade_id": "cell-9b84a698513d4f93",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "sns.barplot(x=X_df.columns, y=theta_hat[1:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1e5074f81fbd04876bf5b37b0d89e482",
     "grade": false,
     "grade_id": "cell-f4555981ba306521",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "This plot shows that quite a few features got a non-zero weight, indicating they either contributed positively or negatively to the predicted survival chance of a passenger. The three most important predictive features seem to be the passenger class, the age of the passenger and whether the passenger was male or female. We cannot directly draw conclusions about the relationship between these features and whether or not a person survived based on this plot, but we *can* use it as a starting point for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "346ec90e8b6f7f48721df33543ccb06f",
     "grade": false,
     "grade_id": "cell-5adb7c0db4bb5bc0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Analyzing the data\n",
    "\n",
    "First, let's take a look at a basic count plot to see how many people have survived overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2f6fd1aa14ca6c4ff73e22b3c743a561",
     "grade": false,
     "grade_id": "cell-1ec7ea7c725dbce4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "sns.countplot(x='Survived',data=data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "074fd182b3f4bdb6b85475a4381f68c4",
     "grade": false,
     "grade_id": "cell-e9c9d3ff1b4f000d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can see from this plot that approximately 350 people have survived, while approximately 550 people have died overall.\n",
    "\n",
    "Now, let's take a look at those 3 features specifically, and see how they affect the distribution.\n",
    "\n",
    "### 1. Gender of the passenger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c0acbd94d1b887e0a8f59e4ef91970a0",
     "grade": false,
     "grade_id": "cell-ba9b1d6ae2c9c1fd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "sns.countplot(x='Survived',hue='Sex',data=data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "54479717b71157b2809ce644ea0fed54",
     "grade": false,
     "grade_id": "cell-51ab0ae64d0c2afb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2. Class of the passenger's cabin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "59284a826e475cb77d0968cb35869647",
     "grade": false,
     "grade_id": "cell-4f80a1e49304b4b1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "sns.countplot(x='Survived',hue='Pclass',data=data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f7769283bb17b94a0e035761e67d3dd0",
     "grade": false,
     "grade_id": "cell-462a2c6aa2dc42c7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 3. Age of the passenger\n",
    "\n",
    "This is a continuous variable, so we can't plot all the distinct cases. Instead we'll use a barplot with the averages for both classes, and corresponding error bars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e7a34081a0017ae3ec496f63f5480a61",
     "grade": false,
     "grade_id": "cell-8e0b24f8285019c7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "sns.barplot(x='Survived', y='Age', data=data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c029e5e516ea77108616f834dad45c52",
     "grade": false,
     "grade_id": "cell-d2d0544a1ac133f4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Creating hypotheses for the data\n",
    "\n",
    "For each of the 3 features plotted above, compare their individual impact on the survival chance of a passenger. For every feature, explain if the this feature has a positive or negative effect on the survival chance of a passenger and give your own hypothesis on why this feature is likely to effect survival (you may speculate here). Finally, relate this conclusion back to the positive or negative weight the feature gets in the learned logistic regression model above:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6be3e4613d56bc1da0d2777891d6bcb1",
     "grade": false,
     "grade_id": "cell-cb0407b868522d24",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Q4. Gender of the passenger**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5645de699857fe6b20f4263d2af98ab8",
     "grade": true,
     "grade_id": "cell-03ca7097fa961362",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cf33789483a0dff32408168302769dc7",
     "grade": false,
     "grade_id": "cell-00b28207dad24df5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Q5. Class of the passenger's cabin**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d0106bba507d4d290d19ce818192b7be",
     "grade": true,
     "grade_id": "cell-5aed066579f38b58",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bdbd789d10ce7102697cae0a5f425b91",
     "grade": false,
     "grade_id": "cell-100f5acbd6b5c71a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Q6. Age of the passenger**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b1b45562fb0f800a7e127e8dfd9c0727",
     "grade": true,
     "grade_id": "cell-2c55876b3cddd57a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
