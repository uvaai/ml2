# Logistic Regression

This week we'll continue with more videos from Andrew Ng's machine learning
course on Coursera. The topic this week is Logistic Regression, which is a
regression model that is used for classification problems. The last video here
you've already seen, but we have included it again this week, as it is very
important for the model we will build in this week's programming exercises.

### Logistic Regression: Classification

![embed](https://youtube.com/embed/-la3q9d7AKQ)

### Logistic Regression: Hypothesis Representation

![embed](https://youtube.com/embed/t1IT5hZfS48)

### Logistic Regression: Decision Boundary

![embed](https://youtube.com/embed/F_VG4LNjZZw)

### Logistic Regression: Cost Function

![embed](https://youtube.com/embed/HIQlmHxI6-0)

### Logistic Regression: Simplified Cost Function and Gradient Descent

![embed](https://youtube.com/embed/TTdcc21Ko9A)

### Logistic Regression: MultiClass Classification OneVsAll

![embed](https://youtube.com/embed/-EIfb6vFJzc)

### Recap: Feature Scaling

You have already seen this video in module 5 of ML1. We will discuss and apply
feature scaling in this week's notebook, so if you are unsure on when and why
you would apply feature scaling, please watch this video again.

![embed](https://youtube.com/embed/r5E2X1JdHAU)

### Q & A

Any questions you might have about this theory can be asked 
[here](https://forms.office.com/Pages/ResponsePage.aspx?id=zcrxoIxhA0S5RXb7PWh05ZTDc7biyulCvpu4U-tarWtUMlZYQUlYMFVMREdWRVVPWTNITlIxQlFUTC4u).
These questions will be answered during the Q&A lecture of this module.

