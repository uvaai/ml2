{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fbfba48294b1c4da479e3657c5650058",
     "grade": false,
     "grade_id": "cell-e61ca27a079c37a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Getting started with neural networks\n",
    "\n",
    "For this assignment, we're going to take a first look at neural networks. Neural networks are a model constructed of a large combination of artificial neurons. We can use logistic regression as the building block for each of these artificial neurons and create a whole network of logistic regression units.\n",
    "\n",
    "Logistic regression is not the only building block we could use, there are a lot of different types of units and connections used in neural networks. However, logistic regression is the block that was historically used in the first neural networks, and in many ways, it is the most intuitive regression unit to create such a network with. So, for this assignment, we'll build some neural networks using only logistic regression units. In the language of neural networks, we're building a fully connected neural network with sigmoid activations at every layer.\n",
    "\n",
    "This notion of combining several elemental building blocks or *modules*, in\n",
    "order to create a more complex network, is at the heart of neural networks. In\n",
    "fact, many of the recent advances in AI come from *deep* neural networks, which\n",
    "means they are the result of combining **a lot** of these modules. So, building\n",
    "the modules in such a way that we can easily stack many of them, will be\n",
    "essential.\n",
    "\n",
    "This assignment consists of three parts. In the first part you will learn more about modularity. You will re-implement logistic regression, but now in such a way that it can be used as a module in a neural network. In the second part you will use this logistic regression module to build a neural network. In the last part you will have a look at how you could implement the training of a single logistic regression module. This last part sets you up for the next module in which you will learn how to deal with the training of a complete neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d2c08169b81d81950eb29968074c06dd",
     "grade": false,
     "grade_id": "cell-b1f20cd20be6c6a1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 1: Modularity (Logistic Regression)\n",
    "\n",
    "The first module we will build, as mentioned, is logistic\n",
    "regression. You've already programmed this algorithm of course, but we'll\n",
    "change the implementation to make it better fit with the idea of modularity.\n",
    "None of these changes will modify the core steps of Logistic\n",
    "regression, but they will change the way we program and mathematically\n",
    "represent the algorithm. The 3 main changes will be:\n",
    "\n",
    "1. Changing the way the bias term $\\theta_0$ is added to the inputs. Up to now\n",
    "now we've always added a column of 1's to the inputs, so the bias parameter\n",
    "could just be handled the same as every other parameter. However, if we stack a\n",
    "lot of modules together, that solution would become very messy.\n",
    "\n",
    "2. Viewing every step of the algorithm as part of a computational graph, even\n",
    "a simple step like adding the bias. This might seem like a strange step at\n",
    "first, but it will make trying to learn the parameters of all these modules a\n",
    "lot easier.\n",
    "\n",
    "3. Implementing the algorithm using *Object Oriented* programming, making it\n",
    "possible to create several *instances* of the algorithm, which can then easily\n",
    "be stacked and linked together, creating a large network of these smaller\n",
    "self-contained modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8cfcff88d1bd467e27a6ca3a04fb9942",
     "grade": false,
     "grade_id": "cell-a3362b9b42af4abc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Logistic Regression, change 1: the bias term \n",
    "\n",
    "#### The classical approach\n",
    "\n",
    "The classical approach to add a bias term in logistic regression is by adding a $1$ to the input $x$ and adding a term $\\theta_0$ to the parameter vector $\\theta$:\n",
    "\n",
    "$$\n",
    "\\tilde{x} =\n",
    "\\begin{pmatrix}\n",
    "1 & x_1 & x_2 & \\cdots & x_n\n",
    "\\end{pmatrix}, \n",
    "\\theta =\n",
    "\\begin{pmatrix}\n",
    "\\theta_0 & \\theta_1 & \\theta_2 & \\cdots & \\theta_n\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "So to compute the output (hypothesis) $\\hat{y}$ you just have to multiply the $\\tilde{x}$ and $\\theta$ vectors before applying the sigmoid function $g$:\n",
    "\n",
    "$$\n",
    "z = \\tilde{x} \\cdot \\theta \\\\\n",
    "\\hat{y} = g(z) \\\\\n",
    "g(z) = \\frac{1}{1+e^{-z}}\n",
    "$$\n",
    "\n",
    "Working these equations gives the following result:\n",
    "\n",
    "$$\n",
    "\\hat{y} = g(\\theta_0 + \\theta_1 \\cdot x_ 1 + \\theta_2 \\cdot x_1 + \\cdots  + \\theta_n \\cdot x_n)\n",
    "$$\n",
    "\n",
    "Where you can see that $\\theta_0$ is the bias value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f94c5f2ba66493bd70bfd2d34c3b6674",
     "grade": false,
     "grade_id": "cell-02d1958d987729c6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Add the bias explicitly\n",
    "\n",
    "This approach becomes messy when we take logistic regression to be a module that can be chained to other modules, like we would for neural networks. So instead we're going to explicitly add the bias term:\n",
    "\n",
    "$$\n",
    "z = x \\cdot \\theta + b\\\\\n",
    "\\hat{y} = g(z)\n",
    "$$\n",
    "\n",
    "When you work out these equations you can see that it is mathematically identical to the previous result, with the only difference that the bias term $\\theta_0$ is now called $b$:\n",
    "\n",
    "$$\n",
    "\\hat{y} = g(\\theta_1 \\cdot x_ 1 + \\theta_2 \\cdot x_1 + \\cdots  + \\theta_n \\cdot x_n + b)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d9eef9a74ae6d1896ab9ddfa7a6438c1",
     "grade": false,
     "grade_id": "cell-d27a4e34c357a7bc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Vectorize with $\\oplus$\n",
    "\n",
    "Although the new representation is mathematically the same as the classic representation, you have to pay attention when vectorizing this. When we want to calculate the output for multiple samples at once, so $X$ is an $m \\times n$ matrix ($n$ features, $m$ samples) you would get the equations:\n",
    "\n",
    "$$\n",
    "z = X \\cdot \\theta \\color{red} + b\\\\\n",
    "\\hat{y} = g(z)\n",
    "$$\n",
    "\n",
    "Here $X \\cdot \\theta$ is a vector containing $m$ values, but $b$ a simple scalar. In linear algebra those *cannot be added*. However numpy has a way around this. It uses a system called broadcasting to apply the addition of $b$ to each of the elements of $X \\cdot \\theta$. We will use the symbol $\\oplus$ to denote numpy style addition. This is operation is strictly speaking not a part of linear algebra and so you will not find it in any literature, but it is very convenient to use in this context. So the vectorized equations will become: \n",
    "\n",
    "$$\n",
    "z = X \\cdot \\theta \\oplus b\\\\\n",
    "\\hat{y} = g(z)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ff0de749f0c192c03508ebb0bd4a388a",
     "grade": false,
     "grade_id": "cell-98d275dae6e51b8a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Gradient terms\n",
    "\n",
    "This also changes the gradient. Or we should say gradients. Next to computing the gradient for $\\theta$ you have to also compute the gradient for $b$. The gradient for $\\theta$ is the same as for the classic approach, but the one for $b$ differs slightly:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J_{\\theta,b}}{\\partial \\theta_j} = \\frac{1}{m}\\sum_{i=1}^m (\\hat{y}^i - y^i)x_j^i \\\\\n",
    "\\frac{\\partial J_{\\theta,b}}{\\partial b} = \\frac{1}{m}\\sum_{i=1}^m (\\hat{y}^i - y^i)\n",
    "$$\n",
    "\n",
    "The derivation of $\\frac{\\partial J_{\\theta,b}}{\\partial b}$ is a bit long for this notebook. But, the result shouldn't be too surprising, after all the bias term in the original solution was computed by setting $x_0$ to $1$ and when you substitute $x_j$ by $1$ in the first equation above, you get the second.\n",
    "\n",
    "When represented as matrix operations, this comes down to:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J_{\\theta,b}}{\\partial \\theta} = \\frac{1}{m}X^T(\\hat{y} -y) \\\\\n",
    "\\frac{\\partial J_{\\theta,b}}{\\partial b} = \\frac{1}{m}\\sum_{i=1}^m (\\hat{y}^i - y^i)\n",
    "$$\n",
    "\n",
    "> Note that the gradient of $b$ isn't reduced any further into linear algebra terms. It is possible to do so, but it isn't very helpful for implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "47db5ab69f653cfb9b3d0dacccf58d2f",
     "grade": false,
     "grade_id": "cell-2435918f042a4ab6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Logistic Regression, change 2: computational graphs\n",
    "\n",
    "\n",
    "#### Computational graphs in general\n",
    "\n",
    "A convenient way to represent the mathematics in neural networks is using computational graphs. It allows you to represent a neural network as data flowing through a network of computation. Especially when we're going to look at backpropagation (i.e. learning), this is particularly useful. We will already start using computational graphs here, to get used to the notation.\n",
    "\n",
    "The following graph represents the computation $c= a + b$\n",
    "\n",
    "<img src=\"src/cg1.svg\" width=\"20%\">\n",
    "\n",
    "The computation $+$ is represented as a node where the data from variables $a$ and $b$ flow into.\n",
    "\n",
    "So if the values of $a$ and $b$ are $2$ and $1$, respectively the data-flow through the computational graph would look like this:\n",
    "\n",
    "<img src=\"src/cg2.svg\" width=\"20%\">\n",
    "\n",
    "Let's have a look at a second, more complex example, $c = ln(ab + 2a^2)$:\n",
    "\n",
    "<img src=\"src/cg3.svg\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d02a937c0e508265b184e3a70bc5a3c1",
     "grade": false,
     "grade_id": "cell-bd91c429eb6aeae8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Logistic regression as computational graph\n",
    "\n",
    "Similarly we can represent logistic regression as a computational graph. We adopt the convention to represent the non vectorized version of the equations for the computational graph. So we will assume a single input vector $x$ and a single output value $y$.\n",
    "\n",
    "The computational graph for logistic regression looks like this:\n",
    "\n",
    "<img src=\"src/Logistic Regression.svg\" width=\"40%\">\n",
    "\n",
    "And we represent the gradients by dashed arrows going in the opposite direction:\n",
    "\n",
    "<img src=\"src/Logistic Regression gradients.svg\" width=\"40%\">\n",
    "\n",
    "With the relative low complexity of logistic regression, this representation might not feel very useful yet. But it will start to make more sense once we move on to neural networks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f566e2ecaf1f9d847145b1c3fa75e12a",
     "grade": false,
     "grade_id": "cell-0f994012c73af2df",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Logistic Regression, change 2: implemented as a python class\n",
    "\n",
    "#### Gradient descent algorithms\n",
    "\n",
    "Before we implement logistic regression regression as a class, it is good to think about it's specifications. All gradient descent algorithms (e.g., linear and logistic regression) follow the same pattern: \n",
    "\n",
    "- They all predict some output $y$ based on some input $X$ (the _forward_ pass).\n",
    "- There is some cost function (loss) that allows us to quantify how good our current predictions are.\n",
    "- They all compute some gradient that tells us how we should change the parameters of the model to give better predictions next time (the _backward_ pass).\n",
    "- There is some form of gradient descent in which we take a step in the opposite direction of the gradient.\n",
    "\n",
    "#### Using classes\n",
    "\n",
    "In the previous modules you implemented these steps for both linear and logistic regression using separate variables and functions. This made running the algorithm and managing the data quite messy. The idea of using a class is that we can bundle all of this together and making running the descent algorithm much cleaner. \n",
    "\n",
    "Before we implement the class let's have a look at how it is *intended to be used*. What we would like is that (once the class is defined) we can create a new logistic regression model and set the initial values of the learning parameters ($\\Theta$ and $b$) by simply creating a new instance. For example, if we want to have a logistic model with 4 inputs:\n",
    "\n",
    "    my_model = Logistic(4)\n",
    "\n",
    "Then, if we want do a step of the gradient descent algorithm for some input `X` and some given output `y`, we want to be able to do something like this:\n",
    "\n",
    "    # predict the output for X\n",
    "    my_model.forward(X)\n",
    "    \n",
    "    # compute the gradients\n",
    "    my_model.backward(y)\n",
    "    \n",
    "    # update the learning parameter theta and b, using a learning rate alpha\n",
    "    my_model.step(alpha)\n",
    "\n",
    "Note that nowhere above you see any variables representing the learning parameters or the gradients. This is the crucial part: they are encapsulated inside the class. So once the class is define we don't have to think about them anymore. Those details will be abstracted away by our design. \n",
    "\n",
    "The function `optimize()` is the function that will run the gradient descent given the model that you will implement. The function `optimize()` is already implemented a little bit further down. Have a look at it, you will see it implements the logistic algorithm exactly as described here. However, it will not work yet, because the implementation of class `Logistic` is not fished yet. This is left up to you to finish.\n",
    "\n",
    "#### Class specification\n",
    "\n",
    "As we established, all gradient descent algorithms are very similar and so the classes implementing them will all look very similar. We can rely on the previously mentioned commonalities to define a general template for any of those algorithms. We will agree that any class that represents a gradient descent module, will implement the following methods:\n",
    "\n",
    "- `__init__`, set initial learning parameter.\n",
    "- `forward`, given some input $x$ predict an output $\\hat{y}$.\n",
    "- `backward`, compute all gradients of the parameters (given the output $\\hat{y}$ computed in `forward`, and the target value $y$).\n",
    "- `step`, update all parameters (based on the gradients computed in `backward`, and a learning rate `alpha`).\n",
    "- `cost`, compute the cost (given the output $\\hat{y}$ computed in `forward`, and the target value $y$).\n",
    "\n",
    "By observing this template we should be able to apply the gradient descent algorithm to any module by repeatedly calling the methods `forward`, `backward` and `step` in succession, until we reach convergence (whatever our criterion for convergence might be).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "68f08130ba871b29afcf93be5c2a3d75",
     "grade": false,
     "grade_id": "cell-f5a3c25f51774e0e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 1\n",
    "\n",
    "Now we're going to re-implement logistic regression as a class. We've made a start with the implementation, but the class `Logistic` below is not completely finished. Your task is to complete the `TODO`'s. Of course, you can re-use some code from the logistic regression assignment from last week, but bear in mind that there are some changes in implementation regarding the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9c29422db907ae966fa3cdf3da3d431c",
     "grade": false,
     "grade_id": "cell-949db3489a20fe97",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Logistic():\n",
    "    def __init__(self, s_in):\n",
    "        \"\"\" Set initial values of parameters. \"\"\"\n",
    "        # the parameters (updated in step)\n",
    "        self.theta = np.zeros(s_in)\n",
    "        self.b = 0\n",
    "        \n",
    "        # the input and output values (set in forward)\n",
    "        self.X = None\n",
    "        self.y_hat = None\n",
    "        \n",
    "        # the gradient values (set in backward)\n",
    "        self.d_theta = None\n",
    "        self.d_b = None\n",
    "  \n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\" Compute prediction, y_hat, based on self.theta, self.b and x.\"\"\"\n",
    "        self.X = X\n",
    "        \n",
    "        # predict (compute self.z and self.y_hat)\n",
    "        # TODO: your code here\n",
    "        \n",
    "        return self.y_hat\n",
    "        \n",
    "    def backward(self, y):\n",
    "        \"\"\" Compute gradients self.d_theta and self.d_b, based on self.y_hat and\"\"\"\n",
    "        self.y = y\n",
    "        \n",
    "        # compute gradients (self.grad_theta)\n",
    "        # TODO: your code here\n",
    "    \n",
    "    def step(self, alpha = 0.1):\n",
    "        \"\"\" Update self.theta and self.b based on self.d_theta, self.d_b, and alpha.\"\"\"\n",
    "        # single gradient descent step (update self.theta)\n",
    "        # TODO: your code here\n",
    "        \n",
    "    def cost(self, y = None):\n",
    "        \"\"\" Compute cost, based on prediction, self.y_hat, and target, y.\"\"\"\n",
    "        if not y:\n",
    "            y = self.y\n",
    "            \n",
    "        # return cost based on:\n",
    "        # - the predicted output (self.y_hat) \n",
    "        # - and the actual values (y)\n",
    "        # TODO: your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5bfd212942c1b50cff31843020a44a82",
     "grade": false,
     "grade_id": "cell-0988c34a857ee1e1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The function `optimize` below runs the gradient descent algorithm. If you implemented the methods above correctly, this function should work for the `Logistic` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "80b56b9d23bad65fa968606599386b47",
     "grade": false,
     "grade_id": "cell-478f0a7ed43cc59c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def optimize(model, X, y, alpha, iterations = 500):\n",
    "    \"\"\"Apply gradient descent to any model that inherits the Module class.\"\"\"\n",
    "    costs = []\n",
    "    for i in tqdm(range(iterations)):\n",
    "        # descent\n",
    "\n",
    "        model.forward(X)        \n",
    "        model.backward(y)\n",
    "        model.step(alpha)\n",
    "        \n",
    "        # keep track of costs\n",
    "        costs.append(model.cost())\n",
    "        \n",
    "        # check for divergence (alpha too big)\n",
    "        if len(costs) >= 2 and (costs[-2] - costs[-1]) < 0:\n",
    "            print(f'Diverging at iteration {len(costs)}')\n",
    "            return costs  \n",
    "    return costs\n",
    "\n",
    "def confusion_matrix(p, y):\n",
    "    return np.vstack((p, 1 - p)) @ np.vstack((y, 1 - y)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "201da9dfe5cb714ec0054fa3364ff339",
     "grade": false,
     "grade_id": "cell-e431931347410616",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Test your solution\n",
    "Let's start by loading the titanic data again. This time we start with a version of this dataset that is already cleaned up and ready to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d4623ca1bdfba189f5ba6d2945568798",
     "grade": false,
     "grade_id": "cell-be1b144451e8688f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def round_output(x):\n",
    "    return (x >= 0.5) * 1\n",
    "\n",
    "data = pd.read_csv('data/clean_titanic.csv', index_col = 0)\n",
    "y = data['Survived']\n",
    "X = data.drop('Survived', axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, test_size = 0.3,  random_state=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f65529d5c9e5760a1e3acab7e71e21c2",
     "grade": false,
     "grade_id": "cell-c4b3154a46240e5b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The code below runs the gradient descent and shows the confusion matrix.\n",
    "\n",
    "If all went well, the confusion matrix should look the same to the one you found in the previous assignment for logistic regression. The values should be close to this:\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "& y=1 & y=0 \\\\\n",
    "\\hat{y}=1 & 69 & 21 \\\\\n",
    "\\hat{y}=0 & 23 & 155\n",
    "\\end{matrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b91e30b71d02dbc66a616c051df8dfc",
     "grade": false,
     "grade_id": "cell-185b233c22dac50a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# train model and predict\n",
    "logistic_model = Logistic(X_train.shape[1])\n",
    "optimize(logistic_model, X_train, y_train, 0.2)\n",
    "predictions = round_output(logistic_model.forward(X_test))\n",
    "\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(predictions, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a18cd9dc831027e9ed59c19e9fd8fe10",
     "grade": false,
     "grade_id": "cell-67e1d9dbff5fec1b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 2: Neural networks\n",
    "\n",
    "### Logistic layer\n",
    "\n",
    "As you have learned from Andrew's videos, classic neural networks consist of multiple logistic layers. Each layer is essentially logistic regression but instead of one single output, a logistic layer can have multiple outputs. And those outputs serve as the inputs for the next layer.\n",
    "\n",
    "#### Intuitive solution\n",
    "So let's say, we want to create a logistic layer that outputs $o$ values labeled $y_1, y_2, \\ldots y_o$. One way you could theoretically accomplish this by just creating $o$ different logistic regression units with $o$ individual $\\theta$ parameters, and $o$ individual $b$ parameters, that all operate on the input $x$, like so:\n",
    "\n",
    "<img src=\"src/Mulit Logistic Regression.svg\" width=\"50%\">\n",
    "\n",
    "Where we could create the output by composing all the outputs of the individual units into one vector: \n",
    "\n",
    "$$\n",
    "\\hat{y} = \\begin{pmatrix} y_1 & y_2 & \\cdots & y_o \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "#### Efficient solution\n",
    "\n",
    "This makes intuitive sense but it is not particularly elegant, and it will not be very efficient if you would implement it this way. Generally we always try to express everything as much as possible as a single matrix operation. This uses your the hardware you use for computation much more efficiently. \n",
    "\n",
    "We can rewrite the above graph by creating a single weight matrix combining all the weights $\\theta_i$ and a vector $b$ containing all the biases $b_i$:\n",
    "\n",
    "$$\n",
    "W = \n",
    "\\begin{pmatrix} \n",
    "-\\theta_1 - \\\\ \n",
    "-\\theta_2 - \\\\ \n",
    "\\vdots \\\\\n",
    "-\\theta_o -  \n",
    "\\end{pmatrix}\n",
    ",\n",
    "b = \n",
    "\\begin{pmatrix} \n",
    "b_1 \\\\ \n",
    "b_2 \\\\ \n",
    "\\vdots \\\\\n",
    "b_o\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "The resulting computational graph:\n",
    "\n",
    "<img src=\"src/cg4.svg\" width=\"40%\">\n",
    "\n",
    "In the form of equations:\n",
    "\n",
    "$$\n",
    "z = W \\cdot x + b\\\\\n",
    "\\hat{y} = g(z)\n",
    "$$\n",
    "\n",
    "Which are essentially the same equations as for logistic regression. But, in stead of the vector $\\theta$ we have a $o\\times n$ weight matrix called $W$ (for an input containing $n$ features and $o$ output values). And the bias $b$ is not a scalar value, but a vector of size $o$.\n",
    "\n",
    "> You've seen two possible solutions: The composed logistic regression units and the representation as a single mathematical system. Verify for yourself that they are mathematically equivalent!\n",
    "\n",
    "> Note that Andrew use slightly different conventions here: He use the symbol $\\Theta$ for the weight matrix. And he doesn't use a separate $b$ vector, but instead relies on the trick of augmenting the input vector with $x_0 = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3a9402e5b9361c50a216824e2953dacd",
     "grade": false,
     "grade_id": "cell-3df9179e18d3a54d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Logistic layer vectorized\n",
    "\n",
    "We have to pay attention when we want to compute multiple samples at once. When $X$ is an $m \\times n$ matrix ($n$ features, $m$ samples), we get the following computational graph:\n",
    "\n",
    "\n",
    "Where $\\hat{Y}$ and $Z$ are $m \\times o$ matrices ($o$ outputs, $m$ samples), $b$ is vector containing $k$ bias terms and $W$ is a $o \\times n$ matrix. The main difference with logistic regression is that you need to transpose the weight matrix in order to keep the dimensions correct:\n",
    "$$\n",
    "Z = X \\cdot W^{T} \\oplus b\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ddcce815ccc1852d02d68b275d718142",
     "grade": false,
     "grade_id": "cell-eb115918f19a19bd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Example\n",
    "So if we have a logistic layer with 2 inputs and 3 outputs, with the following equations:\n",
    "\n",
    "$$\n",
    "Z = X \\cdot W^{T} \\oplus b\\\\\n",
    "\\hat{Y} = g(Z)\n",
    "$$\n",
    "\n",
    "The values are represented by the following matrices (for an input containing $m$ samples):\n",
    "\n",
    "$$\n",
    "X = \n",
    "\\begin{pmatrix}\n",
    "x_{1,1} & x_{1,2} \\\\ \n",
    "x_{2,1} & x_{2,2} \\\\ \n",
    "\\vdots & \\vdots \\\\\n",
    "x_{m,1} & x_{m,2} \\\\ \n",
    "\\end{pmatrix}\n",
    ",\n",
    "\\hat{Y} = \n",
    "\\begin{pmatrix}\n",
    "\\hat{y}_{1,1} & \\hat{y}_{1,2} & \\hat{y}_{1,3} \\\\ \n",
    "\\hat{y}_{2,1} & \\hat{y}_{2,2} & \\hat{y}_{2,3} \\\\ \n",
    "\\vdots & \\vdots  & \\vdots \\\\\n",
    "\\hat{y}_{m,1} & \\hat{y}_{m,2} & \\hat{y}_{m,3}\n",
    "\\end{pmatrix}\n",
    ",\n",
    "W = \n",
    "\\begin{pmatrix}\n",
    "W_{1,1} & W_{1,2} \\\\ \n",
    "W_{2,1} & W_{2,2} \\\\ \n",
    "W_{3,1} & W_{3,2} \n",
    "\\end{pmatrix}\n",
    ",\n",
    "b = \n",
    "\\begin{pmatrix}\n",
    "b_{1} & b_{2} & b_{3}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "If you enter those values into the equations, you will get the following output:\n",
    "\n",
    "$$\n",
    "\\hat{Y} = \n",
    "\\begin{pmatrix}\n",
    "g(W_{1,1}x_{1,1} + W_{1,2}x_{1,2} + b_1) & g(W_{2,1}x_{1,1} + W_{2,2}x_{1,2} + b_2) & g(W_{3,1}x_{1,1} + W_{3,2}x_{1,2} + b_3)\\\\ \n",
    "g(W_{1,1}x_{2,1} + W_{1,2}x_{2,2} + b_1) & g(W_{2,1}x_{2,1} + W_{2,2}x_{2,2} + b_2) & g(W_{3,1}x_{2,1} + W_{3,2}x_{2,2} + b_3) \\\\ \n",
    "\\vdots & \\vdots  & \\vdots \\\\\n",
    "g(W_{1,1}x_{3,1} + W_{1,2}x_{3,2} + b_1) & g(W_{2,1}x_{3,1} + W_{2,2}x_{3,2} + b_2) & g(W_{3,1}x_{3,1} + W_{3,2}x_{3,2} + b_3)\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Verify this for yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6954fcbde896b4f55ae8505e35a6bff0",
     "grade": false,
     "grade_id": "cell-b8d7949f80c6babb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 2\n",
    "\n",
    "Implement the class `LogisticLayer` below. You will only have to implement the `init` and the `forward` method. Note that the `forward` method should be very similar to the one of logistic regression. Following the mathematics outlined above, the forward method should implement the following computational graph:\n",
    "\n",
    "<img src=\"src/cg5.svg\" width=\"40%\">\n",
    "\n",
    "The `init` method should set the $W$ and $b$ parameters (`self.W` and `self.b`) as `np.array`'s of the right dimensions, filled with zeros as provided by `s_in` (the input size) and `s_out` (the output size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "04f7a3cf002fe890bfb8e0264cf94a22",
     "grade": false,
     "grade_id": "cell-f502a850337d44e3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class LogisticLayer():\n",
    "    def __init__(self, s_in, s_out):\n",
    "        \"\"\" Set initial values of parameters. \"\"\"\n",
    "        # TODO: your code here\n",
    "        \n",
    "    def manually_set_weights(self, W, b):\n",
    "        \"\"\" Set weights manually. Normally you wouldn't do this, but usefull for exercises. \"\"\"\n",
    "        print(W, self.W)\n",
    "        assert self.W.shape == W.shape, \"W: wrong shape\"\n",
    "        assert self.b.shape == b.shape, \"b: wrong shape\"\n",
    "        \n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\" Compute prediction, y_hat, based on self.theta, self.b and X.\"\"\"\n",
    "        # TODO: your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3bafb68cf2a0e5ed593bd3c602a33e64",
     "grade": false,
     "grade_id": "cell-728a23770532ff4d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Test if it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d36562cde69870b9b8f26723fe0d2621",
     "grade": true,
     "grade_id": "cell-692b8ef24379540b",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test Logistic Layer\n",
    "testLL = LogisticLayer(3,2)\n",
    "\n",
    "# some arbitrary test values\n",
    "testX = np.array([[-1,  0,  1]])\n",
    "testY = np.array([[0.26894142, 0.73105858]])\n",
    "\n",
    "# set weights\n",
    "testLL.manually_set_weights(np.array([[1, 1, 1], [0, 0, 0]]), np.array([-1, 1]))\n",
    "\n",
    "np.testing.assert_allclose(testLL.forward(testX), testY)\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d0b80e00b2e4b2d443f1e37c078bf4be",
     "grade": false,
     "grade_id": "cell-86c44b2427810801",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Representing logic functions\n",
    "\n",
    "The function above can now produce a network output for any $k$ number of neurons, using a single matrix multiplication, with just those slightly modified Logistic regression functions from before. The only thing needed to produce $k$ outputs is a weight matrix $\\Theta$ of the correct dimensions, and then the result should be a hypothesis matrix $H$ with dimensions $m \\times k$.\n",
    "\n",
    "### Assignment 3\n",
    "\n",
    "In the theory videos, Andrew describes how to encode the boolean *AND* function in neural network weights. This is of course a not very realistic toy example. You wouldn't use neural networks for logic operations, but it can be interesting to look at this example to get a feel for how neural networks work. \n",
    "\n",
    "Recreate Andrew's example using the `LogisticLayer` class to build a really simple network called `andLL`, with 2 inputs and 1 output (i.e. just basic logistic regression). Andrew's example assumes that the bias is part of the $\\Theta$ matrix, whereas we split that up into a separate weight matrix ($W$) and bias vector ($b$). So your solution will look a bit different from the one Andrew proposes.\n",
    "\n",
    "- Make sure $W$ is a matrix of the correct shape and has the correct values for each of the required $W_{ji}$.\n",
    "- Make sure the $b$ is a vector of the correct size.\n",
    "- The network output should be a *column* vector of 4 outputs, corresponding to the logical *AND* of each of the 4 inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "224d45c29efb42f79a06ec919fcd77e3",
     "grade": false,
     "grade_id": "cell-f814f9b203f7773a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "andY = np.array([[0], [0], [0], [1]])\n",
    "\n",
    "# TODO: your code here\n",
    "\n",
    "print(round_output(andLL.forward(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "42283bde7365a903c1b0dd22fd033a02",
     "grade": false,
     "grade_id": "cell-dae0683aeb5cbc98",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f80ab4b78ba4755ecf01a0e163803289",
     "grade": true,
     "grade_id": "cell-4b8a17ad79da5bf6",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_array_equal(round_output(andLL.forward(X)), andY)\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "015673c0180769f5726140038651ed46",
     "grade": false,
     "grade_id": "cell-d69d93a1e586d8d2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assigment 4\n",
    "\n",
    "In the same spirit we can encode some other logical operations. Encode the logical operations *OR* and *NAND* below with neural networks `orLL` and `nandLL` respectively. If you're not sure of their definitions, the expected oupput is provided.\n",
    "\n",
    "Hint: Think how the input corresponds to the output and work your way back. For which output do you need $z$ to be (very) high, and for which output do you need $z$ to be very low. How does this correspond to the input? How can you manipulate $W$ and $b$ to achieve this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ac51db4bfaec9a16febf5d26cdfd3e6",
     "grade": false,
     "grade_id": "cell-f54b68efea82e510",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "orY = np.array([[0], [1], [1], [1]])\n",
    "nandY = np.array([[1], [1], [1], [0]])\n",
    "\n",
    "# TODO: your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "199a06c5d8ad88af62e715acd40e93ad",
     "grade": true,
     "grade_id": "cell-8a88ac25456bba7c",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_array_equal(round_output(orLL.forward(X)), orY)\n",
    "np.testing.assert_array_equal(round_output(nandLL.forward(X)), nandY)\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a1ebff59019bd964faf8c19dc3c39628",
     "grade": false,
     "grade_id": "cell-701963f9a602cb60",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 5\n",
    "\n",
    "Of course, the a logistic layer can have more than one output. Create a logistic layer called `nand_orLL` that has two outputs, corresponding the *NAND* and *OR* values for every input respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6f579f45f3bf22453605f1b289d412ba",
     "grade": false,
     "grade_id": "cell-766985420c457b0c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "nand_orY = np.array([[0, 1], [1, 1], [1, 1], [1, 0]])\n",
    "\n",
    "# TODO: your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b0f06eeddf4d750d87d6ab69a02cb845",
     "grade": false,
     "grade_id": "cell-f570f3425a0a6481",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "68b2b86f7dfe8f6fa066808ee82fad46",
     "grade": true,
     "grade_id": "cell-7db7ad00b8aa8e6b",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_array_equal(round_output(nand_orLL.forward(X)), nand_orY)\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "52e4defa96e33591ef778bb33e6ec9a5",
     "grade": false,
     "grade_id": "cell-0fedcb18d1a1d204",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 6\n",
    "\n",
    "Last one: Try to make a network that models the exclusive or (XOR). If you can't get it to work, explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2d042f4099eb3eb73d2e4510dfe3924e",
     "grade": true,
     "grade_id": "cell-aaf6897d957ba8c7",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "xorY = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# TODO: your code here\n",
    "\n",
    "print(round_output(xorLL.forward(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8a70604e6dfb7bb586a75d9450e68b43",
     "grade": false,
     "grade_id": "cell-b3475c573c2698f2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "It doesn't work? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "910ce673df9d4a3423ca1cc0cb6d9ceb",
     "grade": true,
     "grade_id": "cell-ca61eef0ff80c762",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "89550c08776e4da836cf8cae4fa671f9",
     "grade": false,
     "grade_id": "cell-dcc0550e49bea186",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Multi-layer neural networks\n",
    "\n",
    "For some problems, you need to add a *hidden* layer to the neural network. This is where the real expressive power of neural networks lies; combining layers of the network to represent *non-linear* features and solve complex, non-linear functions.\n",
    "\n",
    "A neural network with 2 layers can already learn *many* more functions than a single layer network. In general, you can stack $n$ different layers together and the basic principle does not change: The output matrix for one layer becomes the input matrix for the next layer, repeated for as many layers as there are in the network.\n",
    "\n",
    "#### Computational graph\n",
    "\n",
    "The computational graph for a 2 layer network looks as follows:\n",
    "\n",
    "<img src=\"src/Multi layer NN.svg\" width=\"70%\">\n",
    "\n",
    "In general, the computational graph for a single layer looks like this:\n",
    "\n",
    "<img src=\"src/Single layer NN.svg\" width=\"35%\">\n",
    "\n",
    "#### The maths\n",
    "\n",
    "Written as equations:\n",
    "$$\n",
    "Z_{i} = A_i \\cdot W_{i}^T \\oplus b \\\\\n",
    "A_{i+1} = g(Z_{i})\n",
    "$$\n",
    "\n",
    "Where the input of layer $i$ is $A_i$ and the output is $A_{i+1}$. For the first layer ($i = 0$) the input is our data $A_{0} = X$. For the last last layer (in the example above $i = 1$) the output is the value we're predicting $A_{2} = \\hat{Y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b40c9917807da4d88dc4130b1fc950d5",
     "grade": false,
     "grade_id": "cell-ef45de451451d89a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Implementation\n",
    "\n",
    "We can now create a new class called `NN` containing a multilayer neural network. Every layer is an `LogisticLayer` object. The `NN` class follows the same template as `LogisticLayer` did, and so in principle this class should also be implementing all the relevant methods (`__init__`, `forward`, `backward`, `step`, and `cost`). However for now we will limit ourselves to only the `__init__` and `forward` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cf497d99f8c89228570ffe3fe12b2bf3",
     "grade": false,
     "grade_id": "cell-60947c0262ef1fe8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 7\n",
    "\n",
    "The `NN` class is partially implemented. It contains an `init` method that given a list with layer dimensions, will create a list with matching `LogisticLayer`objects.\n",
    "\n",
    "Your goal is to add a `forward` method to this class. It should not require you to write any new mathematical code. You should be able to rely on the `forward` of the logistic layers in `self.layers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b716ffed674ba7d5a1e231a4cc4debf1",
     "grade": false,
     "grade_id": "cell-ce857e0ba13db728",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class NN():\n",
    "    def __init__(self, layer_sizes = [2,2,1]):\n",
    "        \"\"\" Set initial layers. \"\"\"\n",
    "        self.layers = []\n",
    "        for s_in, s_out in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
    "            self.layers.append(LogisticLayer(s_in, s_out))\n",
    "            \n",
    "    def manually_set_weights(self, Ws, bs):\n",
    "        \"\"\" Provide list of weight matrices Ws and list of bias vectors bs. Normally you wouldn't do this, but usefull for exercises. \"\"\"\n",
    "        assert len(Ws)==len(self.layers), \"Ws: wrong length\"\n",
    "        assert len(bs)==len(self.layers), \"bs: wrong length\"\n",
    "        \n",
    "        for layer, W, b in zip(self.layers, Ws, bs):\n",
    "            layer.manually_set_weights(W, b)\n",
    "        \n",
    "    # TODO: your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7acac3cde5258b6f29c3b37c9574a0b0",
     "grade": false,
     "grade_id": "cell-6b339d15210e26be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's see if it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "18755cede5843c67a6c8d02ad1fc377f",
     "grade": true,
     "grade_id": "cell-91fff74d8978ae9f",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "testNN = NN(layer_sizes = (3,2,2))\n",
    "\n",
    "# layer 0: 3(+1) -> 2\n",
    "testNN.manually_set_weights([np.array([[0.5, 1.0, 1.5], [-0.5, -1.0, -1.5]]), \n",
    "                             np.array([[1, 0], [0.5, 0.5]])],\n",
    "                            [np.array([-1, 1]),\n",
    "                             np.array([0, 1])])\n",
    "\n",
    "# output\n",
    "result = testNN.forward(np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0], [1, 0, 1], [1, 1, 0], [1, 1, 1]]))\n",
    "answer = np.array([[0.56683301, 0.81757448],\n",
    "                   [0.65077768, 0.81757448],\n",
    "                   [0.62245933, 0.81757448],\n",
    "                   [0.69372123, 0.81757448],\n",
    "                   [0.59327981, 0.81757448],\n",
    "                   [0.67503753, 0.81757448],\n",
    "                   [0.65077768, 0.81757448],\n",
    "                   [0.70698737, 0.81757448]])\n",
    "\n",
    "np.testing.assert_allclose(result, answer)\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2667ee48eb9aad0742919fc541b529d0",
     "grade": false,
     "grade_id": "cell-194a50cbd4b4cc36",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 8\n",
    "\n",
    "Andrew Ng also discusses in detail how to encode the solution for the *XNOR* problem in neural network weights. The network should have 2 inputs, 2 hidden nodes and 1 output, and the necessary added bias nodes for the input and hidden layer.\n",
    "\n",
    "The same is true for the *XOR* network. Define a neural network called `xorNN` that produces the correct *XOR* ouput.\n",
    "\n",
    "> Hint: you can represent a *XOR* b as a combination of other logic ports: (a *NAND* b) *AND* (a *OR* b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "80119fb225d1c2dd0551f1e44fb26202",
     "grade": false,
     "grade_id": "cell-cbe451bd8111af63",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "xorY = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# TODO: your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bdbcdc30e6a9745d5100c1c85222dbf0",
     "grade": false,
     "grade_id": "cell-1c76b2353c92b8e1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e0e64b67af4bb1e2799713b7df82f81",
     "grade": true,
     "grade_id": "cell-3b5faaefbb75c43c",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_array_equal(round_output(xorNN.forward(X)), xorY)\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5446ffa2120d4524d81b7b8d7b168637",
     "grade": false,
     "grade_id": "cell-738f73c0f622a8f0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Classifying handwritten digits\n",
    "\n",
    "This principle is at the *core* of neural networks; adding layers together can solve *much* more complicated problems than just using one layer. For the last part of this assignment, we'll try this algorithm on something a little more interesting than boolean function, digit recognition.\n",
    "\n",
    "The data for this problem can be found in the file `digits123.csv`. Each row contains 65 values, where the first 64 are grayscale pixel values, and the last value is the class label, corresponding to the digit being shown. The grayscale values are integers ranging from 1 to 16, and using some reshaping, can be reconstructed back into a low-resolution *8x8* image.\n",
    "\n",
    "Below is the code to show the image for one digit, i.e. the input for one sample in the data set. Run the code to see the image. Running this code will randomly select a digit to show, so re-running the cell will show different digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "25dbaa7a9e2b90ad7bf33cd82e8da4f0",
     "grade": false,
     "grade_id": "cell-e14cfc188bbaad11",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "digits = np.loadtxt('data/digits123.csv', delimiter=',', dtype=int)\n",
    "\n",
    "def display_digit(i, digits):\n",
    "    digit_sample = np.ones((8,8))*16 - np.reshape(digits[i, :-1], (8, 8))\n",
    "    plt.imshow(digit_sample, cmap='gray', vmax=16)\n",
    "    plt.show()\n",
    "    print(\"The label for this digit was:\", digits[i, -1])\n",
    "\n",
    "display_digit(200, digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "17b7cf2a58d7b9b22da3405d7687617b",
     "grade": false,
     "grade_id": "cell-99cf5a79f07ec229",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Digit recognition solution\n",
    "\n",
    "Recognizing these very low-resolution digits, based on the individual pixel values, is probably not a task with a simple linear decision boundary. However, it is actually solvable with just one hidden layer!\n",
    "\n",
    "Learning the weights of a multi-layer neural network is also done using gradient descent, it is just that computing the correct partial derivatives for *all* the parameters is a lot more tricky. The algorithm to do this is called *backpropagation* and is something we will look at next module.\n",
    "\n",
    "For this assignment, you're just provided with the already learned weight matrices for this digit recognition problem. Running the code below will load the included matrix files and create a complete 2 layer network in `digitNN`. The network has 64 inputs, 65 hidden nodes, and 3 outputs, with each output corresponding to one of the 3 possible digits; 1, 2 and 3.\n",
    "\n",
    "For the this last step, you will need to compute the accuracy of this network in predicting the digits in this data set. The network output will be a $542 \\times 3$ matrix, where the first column indicates the probability that sample was a $1$, the second column the probability the sample was a $2$ and the last column the probability that sample was a $3$. Ultimately, the model should classify each sample as whichever digit was *most likely*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "31585e4ec69b2e2bfd8f7ba90a4f68b5",
     "grade": false,
     "grade_id": "cell-b798b56e722cc19b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 9\n",
    "\n",
    "Implement the function `compute_accuracy`, which takes a matrix of network outputs `H` and one-hot encoded set of class labels `Y`, and returns the percentage of samples that was classified correctly by the network.\n",
    "\n",
    "*Hint:* The function [np.argmax](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html) will save you a lot of work when applied correctly to this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f8312f9de564afdfe796dc28f0f7391b",
     "grade": false,
     "grade_id": "cell-81121b39559cc407",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    " # Normalize the values of the pixels to be between 0 and 1\n",
    "X = digits[:, :-1] / 16\n",
    "\n",
    "# Generate one-hot encoding for Y\n",
    "y = digits[:, -1]\n",
    "Y = np.eye(y.max())[y - y.min()]\n",
    "\n",
    "# Load the already backpropagated weights for the network\n",
    "digitNN = NN((64,65,3))\n",
    "L0 = np.load('data/digits_theta_1.npy')\n",
    "L1 = np.load('data/digits_theta_2.npy')\n",
    "digitNN.layers[0].b = L0[:,0]\n",
    "digitNN.layers[0].W = L0[:,1:]\n",
    "digitNN.layers[1].b = L1[:,0]\n",
    "digitNN.layers[1].W = L1[:,1:]\n",
    "\n",
    "def compute_accuracy(H, Y):\n",
    "    # TODO: your code here\n",
    "\n",
    "# Compute the network outputs for the digits\n",
    "digit_outputs = digitNN.forward(X)\n",
    "\n",
    "# print(\"\\nThe network accuracy for these digits was:\")\n",
    "print(compute_accuracy(digit_outputs, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9669aed5884e7342700cf7a6432a4439",
     "grade": false,
     "grade_id": "cell-c7ea7ab2f273a2df",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 3: Learning\n",
    "\n",
    "Next week we're going to learn how you can train neural networks. But let's glance into the future and look at how we can train **a single** logistic layer. For single layer neural networks, learning the parameters, $W$ and $b$, is very similar to learning $\\theta$ and $b$ with logistic regression.\n",
    "\n",
    "#### As a computational graph\n",
    "\n",
    "Just as with logistic regression, we use the cross entropy loss $l$. At every training step we would like to calculate how much every weight in $W$ and $b$ should change to decrease the loss. For this we need to compute the gradients $\\frac{\\partial l}{\\partial W}$ and $\\frac{\\partial l}{\\partial b}$. In the computational graph below we added these gradients:\n",
    "\n",
    "<img src=\"src/Multi layer NN back.svg\" width=\"39%\">\n",
    "\n",
    "#### The maths\n",
    "\n",
    "Since the output of the logistic layer is not a single value, but a vector, the equations for the gradients are slightly different to those of logistic regression. If your work out the math, you will see that the difference boils down to doing an extra summation over the outputs. The details are for next week. Now we will just provide you with the resulting equations:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J_{W,b}}{\\partial W} = \\frac{1}{m}(\\hat{Y} - Y)^T \\cdot X \\\\\n",
    "\\frac{\\partial J_{W,b}}{\\partial b} = \\frac{1}{m}\\sum_{i=1}^m (\\hat{Y} - Y)^{\\mathrm{row} = i}\n",
    "$$\n",
    "\n",
    "where $\\frac{\\partial J_{W,b}}{\\partial b}$ is simply the sum of all the rows of the $m \\times o$ matrix $\\hat{Y} - Y$.\n",
    "\n",
    "You can verify for yourself that this is congruent with logistic regression by taking the output dimension $o = 1$ (i.e., you can take $Y$ and $\\hat{Y}$ to be vectors of size $m$ in stead of $m \\times o$ matrices). If you work out the math for yourself, you will see that the above equations become identical to those of logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3e8e3886bee0a6d3842af98ea57bd18b",
     "grade": false,
     "grade_id": "cell-463e91074827ac8c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 10\n",
    "\n",
    "Use the formulas above to redefine the `LogisticLayer` class here below. The `init` is already given, you can copy the `forward` method from above, and you have to add the missing methods `backward`, `step`, and `cost`. \n",
    "\n",
    "The implementation of `backward` only has to work for a single layer. You can reuse the code from logistic regression from above, but care should be taken in the vectorization. Because the gradient of the loss ($\\hat{Y} - Y$) is a vector in stead of a single value, you might have to make some changes to keep the dimensions correct.\n",
    "\n",
    "> Note that the `init` method randomizes the $W$ and $b$ paremeters. The reason for this will be discussed next week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f33976d0a3c529d38a30f968967dcef2",
     "grade": false,
     "grade_id": "cell-a50659956d204831",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class LogisticLayer():\n",
    "    def __init__(self, s_in, s_out):\n",
    "        \"\"\" set initial weights \"\"\"\n",
    "        np.random.seed(42)\n",
    "        self.W = np.random.randn(s_out, s_in)\n",
    "        self.b = np.random.randn(s_out)\n",
    "    \n",
    "    # TODO: your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1321071f3965e13631e28a71324d85bd",
     "grade": false,
     "grade_id": "cell-bcd5c2728f128f38",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The test below optimizes the network using the function defined in the beginning of this notebook. If you defined the `LogisticLayer` correctly, it should learn the correct weights for the *AND* output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e2ec8a38bd2558c33356be35c7e21bd9",
     "grade": true,
     "grade_id": "cell-b892e820789301a4",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "andY = np.array([[0], [0], [0], [1]])\n",
    "\n",
    "testLL = LogisticLayer(2, 1)\n",
    "optimize(testLL, X, andY, 10, iterations = 1000)\n",
    "predictions = round_output(testLL.forward(X))\n",
    "print(predictions)\n",
    "np.testing.assert_array_equal(predictions, andY)\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "509b1e78d0a2206747146c68a676952d",
     "grade": false,
     "grade_id": "cell-e0bec5ce089c7dea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Assignment 11\n",
    "\n",
    "Use the logistic layer to learn the correct weights for the *OR* output below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "843bc5f22f930f375345716480b62ce1",
     "grade": false,
     "grade_id": "cell-c9b9b2ac4032cd5b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: your code here\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5747c34a3bc032e232236eabec12ae2e",
     "grade": true,
     "grade_id": "cell-2f71a408de0eb292",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_array_equal(predictions, np.array([[0], [1], [1], [1]]))\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
