
## Activation Functions & Stochastic Gradient Descent

In this notebook we'll continue with a slightly modified version of the neural
network from the previous part. We'll expand it to include ReLU and Softmax
activations, and modify gradient descent to also work with mini-batches.

Download the zip with notebook and data [here](NN_activations.zip).

