# Building Deeper Networks

### Activation functions

![embed](https://youtube.com/embed/Xvg00QnyaIY)

### Why non-linear activation functions

![embed](https://youtube.com/embed/NkOv_k7r6no)

### Derivatives of activation functions

![embed](https://youtube.com/embed/P7_jFxTtJEo)

### Softmax regression

![embed](https://youtube.com/embed/LLux1SW--oM)

### Training a softmax classifier

![embed](https://youtube.com/embed/ueO_Ph0Pyqk)

### The problem of local optima

![embed](https://youtube.com/embed/fODpu1-lNTw)

### Learning with large datasets

![embed](https://youtube.com/embed/lrAe6457ri4)

### Stochastic gradient descent

![embed](https://youtube.com/embed/W9iWNJNFzQI)

### Mini-batch gradient descent

![embed](https://youtube.com/embed/l4lSUAcvHFs)

### Stochastic gradient descent convergence

![embed](https://youtube.com/embed/G97ZtT8mKXk)

### Q & A

Any questions you might have about this theory can be asked 
[here](https://forms.office.com/Pages/ResponsePage.aspx?id=zcrxoIxhA0S5RXb7PWh05ZTDc7biyulCvpu4U-tarWtUMlZYQUlYMFVMREdWRVVPWTNITlIxQlFUTC4u).
These questions will be answered during the Q&A lecture of this module.

