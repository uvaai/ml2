{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c1b303fd19397c2a6ba71437e49153b8",
     "grade": false,
     "grade_id": "cell-2c5ec1cc101f102c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# The German Traffic Sign Recognition Benchmark\n",
    "\n",
    "For this assignment we'll work with the German Traffic Sign Recognition Benchmark, which is a benchmark for classifying images of different types of traffic signs, for example a stop sign. Clearly this type of classification can be useful for autonomous driving vehicles, so they know to come to full stop at certain intersections. Download the training data for yourself [here](https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB-Training_fixed.zip).\n",
    "\n",
    "Unzip the file and take a look at the images in the different folders there. Also take a look at the *Readme.txt* and make sure you understand the structure of the classification task before starting. Below is a cell to download the training and testing data for this benchmark onto this machine.\n",
    "\n",
    "*Note:* Remember to run this notebook on [Kaggle](https://www.kaggle.com/) or [Google Colab](https://colab.research.google.com) with GPU hardware acceleration enabled! This will make training your network *much* faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1dcf0928ba55bc3955dd59fa19c46e88",
     "grade": false,
     "grade_id": "cell-7dee473b4ba70453",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "![ ! -d 'GTSRB/Training' ] && curl -O https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB-Training_fixed.zip\n",
    "![ ! -d 'GTSRB/Training' ] && unzip -q GTSRB-Training_fixed.zip\n",
    "\n",
    "![ ! -d 'GTSRB/Online-Test-sort' ] && curl -O https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Online-Test-Images-Sorted.zip\n",
    "![ ! -d 'GTSRB/Online-Test-sort' ] && unzip -q GTSRB_Online-Test-Images-Sorted.zip\n",
    "\n",
    "![ -d 'GTSRB/Online-Test-sort/Images' ] && rm -rf GTSRB/Online-Test-sort/Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1766e3f929a549edbc49fb8e8f6b5605",
     "grade": false,
     "grade_id": "cell-20f579f742071025",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Below is some code we've already provided to load the data. It is not required you understand what the functions in this cell do exactly. The functions mostly deal with the slightly more complicated loading of the images, as they're all in different folders and have a `.ppm` format. Additionally, it resizes all images to a standard *32 x 32* size, so they'll all fit into a neural network with the same input layer size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c5e49240defed8b4c9aab2545a3ab2b8",
     "grade": false,
     "grade_id": "cell-f6a10ed1318e82d6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def build_image_path_list(data_dir):\n",
    "    image_path_list = []\n",
    "    for root, dirs, files in list(os.walk(data_dir))[1:]:\n",
    "        image_path_list.extend([(os.path.join(root, f), int(root.rsplit(os.sep, 1)[1]))\n",
    "                                for f in files if f.endswith('.ppm')])\n",
    "    return image_path_list\n",
    "    \n",
    "def load_data(data_dir, size=32):\n",
    "    image_list, target_list = [], []\n",
    "\n",
    "    for image_path, target in build_image_path_list(data_dir):\n",
    "        image_list.append(cv2.resize(cv2.imread(image_path), (size, size)))\n",
    "        target_list.append(target)\n",
    "\n",
    "    return (np.array(image_list), np.array(target_list))\n",
    "\n",
    "train_images, train_labels = load_data(os.path.join('GTSRB', 'Training'))\n",
    "test_images, test_labels = load_data(os.path.join('GTSRB', 'Online-Test-sort'))\n",
    "\n",
    "print(f'Training images loaded: {train_images.shape}')\n",
    "print(f'Training labels loaded: {train_labels.shape}')\n",
    "print(f'Testing images loaded: {test_images.shape}')\n",
    "print(f'Testing labels loaded: {test_labels.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eea953a3806ebe48aaa90b5369c5b3a5",
     "grade": false,
     "grade_id": "cell-5b5bb8cd360e14b2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For this assignment you'll have to build and train a deep convolutional neural network yourself for this GTSRB data set. The goal is simply to get as high as possible accuracy on the provided test set, using only the training set to learn. You may of course reuse any code you might find useful from the CIFAR notebook for this, and you are also free to look up any other functions or classes you might want to try from the [TensorFlow Keras API](https://www.tensorflow.org/api_docs/python/tf/keras/) (which is what we've used for the CIFAR notebook too).\n",
    "\n",
    "As you try different versions of your network, you should briefly document what you've tried for each version in the markdown cell below. You should describe what you've tried, a very brief motivation for why you've tried it and what testing accuracy this version produced.\n",
    "\n",
    "*Hint:* A common practice when designing larger neural networks is to start by looking at the architectures of models with of state of the art performance from benchmarking competitions. You can read a description of one such famous model, VGG16, in the article here: [Understanding VGG16: Concepts, Architecture, and Performance](https://datagen.tech/guides/computer-vision/vgg16/). Even though the input for this problem is much smaller than the VGG16 input, you can still experiment and see if there any ideas or layer configurations that you can reuse to improve the results of your own network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version 1\n",
    "\n",
    "*Your description goes here.*\n",
    "\n",
    "\n",
    "#### Version 2\n",
    "\n",
    "*Your description goes here.*\n",
    "\n",
    "...\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d9955cc1777d8912fbb6336e12079515",
     "grade": true,
     "grade_id": "cell-ef39a5b0496eab42",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
