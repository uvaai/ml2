# Deep Learning

### MIT Introduction to Deep Learning

![embed](https://youtube.com/embed/njKP3FqW3Sk)


### Regularizing your neural network: Regularization

![embed](https://youtube.com/embed/6g0t3Phly2M)

### Regularizing your neural network: Why regularization reduces overfitting

![embed](https://youtube.com/embed/NyG-7nRpsW8)

### Regularizing your neural network: Dropout regularization

![embed](https://youtube.com/embed/D8PJAL-MZv8)

### Regularizing your neural network: Understanding Dropout

![embed](https://youtube.com/embed/ARq74QuavAo)


### Batch Normalization: Normalizing activations in a network

![embed](https://youtube.com/embed/tNIpEZLv_eg)

### Batch Normalization: Fitting Batch Norm into neural networks

![embed](https://youtube.com/embed/em6dfRxYkYU)

### Batch Normalization: Why does Batch Norm work?

![embed](https://youtube.com/embed/nUUqwaxLnWs)

### Batch Normalization: Batch Norm at test time

![embed](https://youtube.com/embed/5qefnAek8OA)


### Practical advice for using ConvNets: Data Augmentation

![embed](https://youtube.com/embed/JI8saFjK84o)

### Q & A

Any questions you might have about this theory can be asked 
[here](https://forms.office.com/Pages/ResponsePage.aspx?id=zcrxoIxhA0S5RXb7PWh05ZTDc7biyulCvpu4U-tarWtUMlZYQUlYMFVMREdWRVVPWTNITlIxQlFUTC4u).
These questions will be answered during the Q&A lecture of this module.

