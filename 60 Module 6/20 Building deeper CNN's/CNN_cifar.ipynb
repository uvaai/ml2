{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a deep convolutional neural network\n",
    "\n",
    "For this last machine learning assignment we'll our complete the journey to modern deep neural networks, using *tensorfow* to define whole layers of the network, instead of writing a single Logistic regression unit by hand as in the first module. For this module we'll look at a few more interesting additions you can make to a neural network and practice using `tensorflow` to build the network.\n",
    "\n",
    "As we arrive at more modern neural networks, we'll also quickly need *a lot* more computing power to optimize all the many weights that make up such a network. Running the assignment below on your own CPU is technically possible, but things will go much faster if you use a GPU (i.e. graphics card) to do the heavy lifting.\n",
    "\n",
    "Having your code run on the GPU using `tensorflow` is pretty straightforward, but does of course require that your computer has a good GPU (not standard and expensive). In addition, you'll need to seperately install in the GPU support for `tensorflow`. If you have a good graphics card and would like to install this yourself, you can try this, but it is not recommended for this assignment. If you'd still like to try, instructions can be found here: https://www.tensorflow.org/install/gpu\n",
    "\n",
    "## Google Colab\n",
    "\n",
    "The easiest way to run your code on a GPU is probably using **Google Colab**, which is what we recommend you also use for this assignment. Google offers computing resources completely for free to anyone with a Google account. A *Colab* is a Jupyter notebook that is not running on your own computer, but one of Google's many cloud computers. In order to run this notebook as a Colab, go to https://colab.research.google.com/\n",
    "\n",
    "On the Colab page, select *Upload*, upload this `.ipynb` file and continue with the assignment there. Running a code cell on the Colab page will now actually run the code on a Google computer and not on your own machine!\n",
    "\n",
    "Next we'll switch to a machine that has access to a GPU. In the menu at the top, select **Runtime** and then **Change runtime type**. Select the GPU hardware accelerator and Save. Now, any code you run will run on a Google GPU, which will make training your neural network much faster!\n",
    "\n",
    "To check that everything works, run the code cell below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything is working correctly, you should see a single GPU listed as the device for this machine.\n",
    "\n",
    "Google is offering quite an expensive resource to people for free, so there are some restrictions you should take into account. If your are inactive in the Colab for a while, you will get disconnected from the machine and it will be reassigned to someone else. If you use too much memory, either RAM or harddisk, you'll also get disconnected and have to reconnect to a new machine.\n",
    "\n",
    "Note that any time you get disconnected, switch runtime or restart the runtime, you will have to reconnect to a new machine. When this happens the whole notebook will be restarted, so you'll have to run any code cells, that for example load the data, again. Having to restart and rerun the cells can be a bit annoying, but it is small inconvience for getting free access to a GPU.\n",
    "\n",
    "## CIFAR-10 dataset\n",
    "\n",
    "Speaking of loading the data, run the cell below to download the CIFAR-10 dataset on this machine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "!tar -xzvf cifar-10-python.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CIFAR-10 dataset is a classic dataset for deep learning and was originally created by Alex Krizhevsky. Start by reading the description of the dataset here: https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "\n",
    "Take a look at the format of the images and the random samples from the 10 classes shown on the page there, and make sure you understand the classification task from the dataset before continuing.\n",
    "\n",
    "Below is some code to further unpack and prepare these 60.000 images. There are a couple of important things done as part of loading the data:\n",
    "\n",
    "* Each image is reshaped back to 32 x 32 pixels and 3 color channels using [np.reshape](https://numpy.org/doc/stable/reference/generated/numpy.reshape.html) \n",
    "* Each of the labels is transformed into a *one-hot encoding* using the [to_categorical](https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical) function.\n",
    "* The 5 seperate files of training data are merged to a single *Numpy* array using [np.vstack](https://numpy.org/doc/stable/reference/generated/numpy.vstack.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "def unpickle(filename):\n",
    "    with open(filename, 'rb') as fo:\n",
    "        data = pickle.load(fo, encoding='bytes')\n",
    "        images = [img.reshape((32,32,3), order='F') for img in data[b'data']]\n",
    "        \n",
    "        labels = tf.keras.utils.to_categorical(data[b'labels'])\n",
    "        return (np.array(images), labels)\n",
    "\n",
    "\n",
    "train_images, train_labels = [np.vstack(tr) for tr in zip(*[\n",
    "        unpickle(f'cifar-10-batches-py/data_batch_{i+1}') for i in range(5)])]\n",
    "\n",
    "test_images, test_labels = unpickle('cifar-10-batches-py/test_batch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation function\n",
    "\n",
    "Next up we'll provide a function that you can use for training your neural networks. This function takes a neural network `model`, training and validation data, and does a couple of things:\n",
    "\n",
    "* Compiles the model using\n",
    "    * a [CategoricalCrossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy) loss function. This is the same classification error function we used in combination with the Softmax activation.\n",
    "    * an additional metric of [Accuracy](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Accuracy), to see how well the predictions match the actual labels.\n",
    "    * an [Adam](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam) optimizer, which is a more advanced version of the gradient descent algorithm we've used up to this point to optimize the weights.\n",
    "* Creates a preprocessing [ImageDataGenerator](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator) for the training and validation data.\n",
    "* Fits the compiled model using the preprocessed training data for the specified number of *epochs*.\n",
    "* Plots the loss and accuracy for the training and validation data for each epoch.\n",
    "* Prints the final validation accuracy.\n",
    "\n",
    "We'll dive more deeply into what the preprocessing steps can do exactly in later sections. For now, no preprocessing is applied. Make sure you understand what all the other steps do before moving on to the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras import layers, models, preprocessing\n",
    "\n",
    "def train_and_evaluate(model, train_x, train_y, val_x, val_y, preprocess={}, epochs=20, augment={}):\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    train_gen = preprocessing.image.ImageDataGenerator(**preprocess, **augment)\n",
    "    train_gen.fit(train_x) \n",
    "\n",
    "    val_gen = preprocessing.image.ImageDataGenerator(**preprocess)\n",
    "    val_gen.fit(train_x)\n",
    "\n",
    "    history = model.fit(train_gen.flow(train_x, train_y), epochs=epochs, \n",
    "                        validation_data=val_gen.flow(val_x, val_y))\n",
    "\n",
    "    fig, axs = plt.subplots(1,2,figsize=(20,5)) \n",
    "\n",
    "    for i, metric in enumerate(['loss', 'accuracy']):\n",
    "        axs[i].plot(history.history[metric])\n",
    "        axs[i].plot(history.history['val_'+metric])\n",
    "        axs[i].legend(['training', 'validation'], loc='best')\n",
    "\n",
    "        axs[i].set_title('Model '+metric)\n",
    "        axs[i].set_ylabel(metric)\n",
    "        axs[i].set_xlabel('epoch')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Validation Accuracy: {model.evaluate(val_gen.flow(val_x, val_y))[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a basic CNN model\n",
    "\n",
    "Below is a basic Convolutional Neural Network with 2 convolutional layers. The first layer has 32 filters and the second one has 64 filter. After each convolutional layer a max pooling layer is applied. Next, after the convolutional layers a fully connected layer with 256 hidden nodes is applied, and finally a softmax layer with 10 outputs for each of the 10 classes.\n",
    "\n",
    "Using the function from above, this model is then fitted on training data and evaluated on the validation data. You should get a validation accuracy of about *61%*. This means that already *61%* of the validation images is correctely classified as 1 of the 10 possible classes. Clearly, the network is learning something, as a random prediction for the validation set should result in about *10%* accuracy. Looking at the learning curves it also seems like the network is indeed learning something, although there is cleary room for improvement too.\n",
    "\n",
    "Incrementally improving this network will be the core task of this assignment. For each subsequent improvement you should copy the network from the previous cell and make the new improvements in addition to the other improvement already made. You should of course start with the basic model provided here and the goal will be to improve the validation accuracy as much as possible.\n",
    "\n",
    "Part of the assignment will be to analyse the learning curves at each step to see if a specific improvement is likely to increase the validation accuracy. However, there is one important thing to note about these curves, which might be confusing otherwise. The training cost at each epoch is computed as the *average training cost* of all the batches in the epoch. This means for the first few batches the cost will probably be higher and will get lower with subsequent batches. The validation cost is computed after all the batches have been processed, meaning this cost can actually be *lower* than the training cost.\n",
    "\n",
    "Normally, you would expect the training cost to always be lower than the validation cost, but because the training cost is computed as an average (which is more efficient) this might not be the case. The general shape of the training curve, especially for later iterations, will still be correct and so is still useful for analysis in the same way.\n",
    "\n",
    "#### Q1. Explain why you would normally expect the training cost to always be lower than the validation cost.\n",
    "\n",
    "*Your answer goes here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "train_and_evaluate(model, train_images, train_labels, test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing the input data\n",
    "\n",
    "The basic CNN model seems to clearly be overfitting, but before we address that, we'll start by normalizing the input. This is a first step that is important for many ML models, but specifically for deep neural networks, as ReLU activations are only actually non-linear around an input of 0 (where the input switches from 0 to a linear output). This means for the ReLU activations to really be effective, you would want the average combined inputs to end up around 0, which is why *mean centering the data* is so important.\n",
    "\n",
    "You could use something like `np.mean` to compute the average input in all the training images and then subtract that average from every input, but the easiest way is to use the data preprocessor [ImageDataGenerator](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator). To use this together with the `train_and_evaluate()` function, you'll need to use the optional `preprocess` argument, which is [keyword argument dictionary](https://docs.python.org/3/glossary.html#term-argument).\n",
    "\n",
    "Each key in the *keyword argument dictionary* should be a string of the keyword you want to pass to function and the value in the dictionary should be the corresponding parameter value you want to pass to function. In order to mean center the data, we want to enable `featurewise_center`, so that would look like\n",
    "\n",
    "    preprocess = {'featurewise_center': True}\n",
    "\n",
    "Use this keyword argument dictionary to enable mean centering the data as a preprocessing step. In addition, the training data should be normalized to have a standard deviation of 1, for which there is also a keyword argument. The `train_and_evaluate()` function will then apply these same preprocessing steps to both the training and the validation data.\n",
    "\n",
    "Start by copying the model from the cell before and then modify the `train_and_evaluate()` call to apply the mean centering and standard deviation normalization preprocessing to your data. If done correctly, your validation accuracy should increase to about *70%*.\n",
    "\n",
    "#### Q2. Explain why normalizing your image features (i.e. pixels) to have a standard deviation of 1 might also be important for deep neural networks with ReLU activations.\n",
    "\n",
    "*Your answer goes here.*\n",
    "\n",
    "#### Q3. Explain why it is so important you apply the same normalization to your training and validation data. What would happen if the function did not do this?\n",
    "\n",
    "*Your answer goes here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Dropout\n",
    "\n",
    "Looking at the learning curves from the previous cell, it seems like the model is definitely overfitting. An easy and effective way to address this can be adding dropout layers to the network. [Dropout](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout) layers are already included in `tensorflow` and all you need to specify is for this layer is the probability for each input to be dropped (i.e. be set to 0).\n",
    "\n",
    "Copy your code from the previous cell and add dropout layers to network. You should add a Dropout layers before each of the 2 Dense layers, both with a probability of *0.4*, so approximately *40%* of the inputs of these layers will be set to 0! As you want to maintain more of the network activations closers to the network input, only add a Dropout layer before the *last* Conv2D layer, with a lower dropping probability of *0.2*. If done correctly, your validation accuracy should increase to about *76%*.\n",
    "\n",
    "Looking at the learning curves, the network also doesn't seem to be overfitting any more! An important thing to note about the training cost here is that it may become again higher than the validation cost. During training dropout is active and some of the inputs get dropped, but when evaluating the validation data all inputs are used, probably resulting better prediction and thus a lower cost. To get a better estimate of the true training cost, we would need to add a full evaluation of the training data at each epoch too, but then we'd effectively double the computation for training data at each epoch, making the algorithm much slower. The training cost current curve still has a useful shape for analysis, but the true training cost would be lower at every epoch.\n",
    "\n",
    "#### Q4. Explain what part exactly of learning curves from the previous cell showed that the model was overfitting.\n",
    "\n",
    "*Your answer goes here.*\n",
    "\n",
    "#### Q5. Explain in your own words why adding dropout reduced the chance of the model overfitting.\n",
    "\n",
    "*Your answer goes here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the network deeper\n",
    "\n",
    "Now that the network doesn't appear to be overfitting anymore, we can again try to add more complexity to network and try to make it deeper. This should allow the network to learn more high level features and therefore hopefully improve performance further. \n",
    "\n",
    "Start by copying the code from the previous cell and add 2 more Conv2D layers to network; the first one with 128 filters and the second one with 256 filters. The new layers should be inserted after the existing convolutional layers, and both convolutional layers should have a Dropout layer *before* and a MaxPooling layer *after* (i.e. the same structure as the last convolutional layer).\n",
    "\n",
    "As the network is now deeper, it will also take longer to train, so increase the number of epochs in `train_and_evalute()` to *50*. If all steps are done correctly, your validation accuracy should increase to about *80%*.\n",
    "\n",
    "As a last step, after the code cell below is another cell that should print a summary of the `model` you've now constructed, with information about each of the added layers. Also run that cell and study the output produced there.\n",
    "\n",
    "#### Q6. Which layer has the largest total output volume (i.e. total number of nodes)? Explain why this layer has the most nodes.\n",
    "\n",
    "*Your answer goes here.*\n",
    "\n",
    "#### Q7. Which layer has the largest number of trainable parameters  (i.e. weights)? Explain why this layer has the most weights.\n",
    "\n",
    "*Your answer goes here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Your code goes here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Batch Normalization\n",
    "\n",
    "Now that the network is deeper, we might also benefit from applying batch normalization. With the preprocessing step at the start of the this notebook the input features were normalized to have a 0 mean and standard deviation of 1, which helped the network learn mostly on the first layer with ReLU activations. With batch normalization this same type of scaling can be applied at each of the layers.\n",
    "\n",
    "Start by copying the code from the previous cell and add a [BatchNormalization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization) layer after each MaxPooling layer. Effectively, this places the BatchNormalization layers *before* each of the layers with ReLU activations, except for the input layer (which has already been normalized). If done correctly, your validation accuracy should increase to about 84%.\n",
    "\n",
    "#### Q8. Explain in your own words why adding batch normalization improves the results of deep networks.\n",
    "\n",
    "*Your answer goes here.*\n",
    "\n",
    "#### Q9. The BatchNormalization layer also has two learnable parameters, $\\beta$ and $\\gamma$, which start learning at 0 and 1 respectively. Do you expect these parameters to change a lot when using the layer before a ReLU activation? Why or why not?\n",
    "\n",
    "*Your answer goes here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "The last change we'll try to make is using data augmentation, which artificially increases the size of the data by making variations in the training data. Augmentations on the data can be done using the [ImageDataGenerator](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator), which we used for preprocessing earlier.\n",
    "\n",
    "The possible augmenations and the ranges for their values are listed in the documentation of [ImageDataGenerator](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator). Augmenations should be set in the `train_and_evaluate()` function using a separate *keyword argument dictionary*, which is called `augment`. For example, if you wanted to enable horizontal mirroring and a random zoom of maximum *10%* on training images, you would use\n",
    "\n",
    "    augment = {'horizontal_flip': True, 'zoom_range': 0.1}\n",
    "\n",
    "Try to find a combination of augmentations that improve the accuracy on the validation set even further. As the data set will now contain random variations, you'll probably have to increase the number of epochs quite a bit, depending on how many and how large a range of augmentations you make. This means training might actually take a long time, so it is not required to try enough combinations until you find a better validation accuracy, but only that you try and report your findings below. \n",
    "\n",
    "####  Q10. Explain why you would want to apply the augmentation to your training, but not your validation data. What would happen if the function did apply the augmentation to both?\n",
    "\n",
    "*Your answer goes here.*\n",
    "\n",
    "####  Q11. Which augmentations did you try? Which augmentations gave the best results? Can you think of reason why these augmentations specifically would be effective for the CIFAR-10 images?\n",
    "\n",
    "*Your answer goes here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Your code goes here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
